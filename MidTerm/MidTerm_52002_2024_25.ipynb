{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c71a501a-e69f-4db7-871f-905e9861fe4a",
      "metadata": {
        "id": "c71a501a-e69f-4db7-871f-905e9861fe4a"
      },
      "source": [
        "# Midterm 52002 - 2024-2025\n",
        "\n",
        "**General instructions:** The mid-term should be done in pairs. Submit your solutions in the course by uploading the solution files to the course Moodle page by 10.1.2025\n",
        "\n",
        "**.ipynb solution file:** Fill-in the missing code blocks (for parts 1 and 3)and text blocks (for all three parts) in this ipynb notebook, change the name to `MidTerm_52002_2025_25_<ID1>_<ID2>.ipynb` where replace `<ID1>` and `<ID2>` by your ID nuymbers. Run the filled-notebook it in jupyter notebooks/google colab and upload to moodle the **filled** notebook with results (tables, graphs etc.).\n",
        "\n",
        "**In addition**, submit a pdf/html export of the executed notebook with all the output, named `MidTerm_52002_2025_25_<ID1>_<ID2>.html` (or `.pdf`). Failing to submit both files as instructed will lead to a reduction in your midtem grade.\n",
        "\n",
        "\n",
        "**Good luck!**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48TT9I9BGWd9",
      "metadata": {
        "id": "48TT9I9BGWd9"
      },
      "source": [
        "\n",
        "# BigQuery & SQL - 40 points\n",
        "There are 5 sub-questions in this part. Each sub-question is worth 8 points"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b756e1fb-959f-4a1c-b78c-ab714b3f2c52",
      "metadata": {
        "id": "b756e1fb-959f-4a1c-b78c-ab714b3f2c52"
      },
      "source": [
        "BigQuery is Google's serverless data warehouse that enables scalable analysis   \n",
        "over petabytes of data. It is a Platform as a Service (PaaS) that supports querying   \n",
        "using SQL. In this part of the midterm you'll be asked to query BigQuery tables    \n",
        "using its python API. Please use the `sandbox` BigQuery environment and query   \n",
        "only public datasets.\n",
        "\n",
        "**Please note:**\n",
        "Your BigQuery's resources are limited to `1 TB` per user per month - be mindful with how many queries you   \n",
        "execute and try to optimize your queries as much as possible.\n",
        "\n",
        "\n",
        "**We will utilize two datasets about New-York City:**\n",
        "\n",
        "The first one called `new_york_311` is the 311 calls or service requests dataset, which contains resident complaints from 2010 to 2022.\n",
        "\n",
        "The second dataset called `new_york_trees` is the \"2015 Street Tree Census - Tree Data,\" which includes information from the 1995, 2005, and 2015 Street Tree Censuses. This dataset catalogs trees by address and provides details such as species, diameter, and condition.\n",
        "\n",
        "More information on the two datasets can be found [here](https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9) and [here](https://data.cityofnewyork.us/Environment/2015-Street-Tree-Census-Tree-Data/uvpi-gqnh/about_data).   \n",
        "\n",
        "**Guidelines:**  \n",
        "1. Fill this notebook with python commands including SQL queries in the designated places and run it using a jupyter notebook environment (e.g. google colab)\n",
        "2. Write efficient SQL queries and code. Points may be taken off for inefficient queries and code.\n",
        "3. The python API enables us to write code that interacts with BigQuery and convert between the SQL tables to python objects, thus allowing analysis and plotting using python code. You should write your SQL commands within the API. It is recommended to first browse the dataset and run the SQL command manualy in the BigQuery web-browser environment, before copying it to the python notebool.      \n",
        "4. Your plots should be clear, with titles, and with propoer x-y labels.\n",
        "You may use the `matplotlib` library, or `pandas.DataFrame.plot` for your plots.\n",
        "5. When reading BigQuery tables, used the `` symbols to around the table's name, and use as prefix `'Big Query'` and the name of the dataset. For example, to extract a table present in the `new_york_trees` dataset you should use the name\n",
        "`'bigquery-public-data.new_york_trees.<table-name>'` where `<table-name>` is replaced by the name of the table."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9a40f98-4fa1-40f2-8d3a-2cacb87cf149",
      "metadata": {
        "id": "c9a40f98-4fa1-40f2-8d3a-2cacb87cf149"
      },
      "source": [
        "**Necessary Libraries:**  \n",
        "(Do not use any libraries that are not in this list without permission in writing form the course staff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d8b6eba-d97c-424d-aa1a-0d082e3933f3",
      "metadata": {
        "id": "7d8b6eba-d97c-424d-aa1a-0d082e3933f3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib as plt\n",
        "from google.cloud import bigquery\n",
        "from sqlite3 import connect"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LYGolbl4zxOi",
      "metadata": {
        "id": "LYGolbl4zxOi"
      },
      "source": [
        "### Q1: BigQuery client & Initital Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QR9d8etrFl8R",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR9d8etrFl8R",
        "outputId": "d07fb942-3b58-45e3-e37a-6cb0141c903c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authenticated\n"
          ]
        }
      ],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yg2vHgeGz8KF",
      "metadata": {
        "id": "Yg2vHgeGz8KF"
      },
      "source": [
        "Construct a BigQuery client object using the `client` method of the `bigquery` module.  The project name you use here should match the name of the project you open in the BigQuery environment. You should use in both places the same **non-huji** google user name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "or07V0Fpz8vk",
      "metadata": {
        "id": "or07V0Fpz8vk"
      },
      "outputs": [],
      "source": [
        "client = bigquery.Client(project=\"Write here you project's name\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XpET35_D0Gcn",
      "metadata": {
        "id": "XpET35_D0Gcn"
      },
      "source": [
        "Make a simple query displaying five columns of your choice from one of the tables of your choice in the two datasetsin order to check the connection. Limit the number of rows in the displayed output to 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PH2IE4kg0IzJ",
      "metadata": {
        "id": "PH2IE4kg0IzJ"
      },
      "outputs": [],
      "source": [
        "# Your answer here:\n",
        "my_query = \"\"\"\n",
        "SELECT  ... block_id, tree_id, created_at, tree_dbh, status, health\n",
        "FROM ...\n",
        "WHERFE ...\n",
        "... (additional commands)\n",
        "\"\"\"\n",
        "\n",
        "# query_job = client.query(my_query)  # Make an API request.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CO-p0SuE-Z_x",
      "metadata": {
        "id": "CO-p0SuE-Z_x"
      },
      "source": [
        "\n",
        "[Add text explaining your query and the results here]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d3fc0bf-b611-40d4-92dd-020df6ec41c9",
      "metadata": {
        "id": "5d3fc0bf-b611-40d4-92dd-020df6ec41c9"
      },
      "source": [
        "### Q2: Exploratory Data Analysis\n",
        "**Remark:** unless specified otherwise, refer to the following dataset when considering data for trees:\n",
        "`'bigquery-public-data.new_york_trees.tree_census_2015'`\n",
        "\n",
        "2a. How many trees were there in New York in 2015, and how many of these were healthy (whose health status is not `Poor` or `null`)?\n",
        "\n",
        "2b. What are the ten most common tree species in New York?  Run a query that returns a table with the number of trees for each species, the number of healthy trees and the percentage of healthy trees within each species (relative to the total trees of that species).\n",
        "Display the output table with these details only for the ten species with the highest overall counts.\n",
        "\n",
        "2c. Analyze the status distribution of New York City's trees by examining each ZIP code area. Your analysis should reveal the total number of trees per ZIP code along with the percentage of trees in each of the health status categories within each ZIP code. Exclude records with missing data (`zipcode` or `status`) . Show a table with the results for the five ZIP codes with the highest number of total trees.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2808d85-5f31-485b-9b40-0c2ac68c9aa8",
      "metadata": {
        "id": "b2808d85-5f31-485b-9b40-0c2ac68c9aa8"
      },
      "source": [
        "**Answer:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca2f7f4e-fdfc-4a8f-bc50-57e0932085cc",
      "metadata": {
        "id": "ca2f7f4e-fdfc-4a8f-bc50-57e0932085cc"
      },
      "outputs": [],
      "source": [
        "# 2a)\n",
        "# Fill query here inside triple quotes:\n",
        "query2a = \"\"\"\n",
        "SELECT FROM WHERE ...\n",
        "\"\"\"\n",
        "\n",
        "# query_job2a = client.query(query2a)  # Make an API request.\n",
        "\n",
        "# Output and print\n",
        "# df2a = pd.DataFrame(query_job2a.result().to_dataframe())\n",
        "# df2a.head()\n",
        "\n",
        "# You may make additional queries and display their results here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FNZxz2bzNw_M",
      "metadata": {
        "id": "FNZxz2bzNw_M"
      },
      "source": [
        "\n",
        "[Text for your answer here]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YB1Q-oRNAKle",
      "metadata": {
        "id": "YB1Q-oRNAKle"
      },
      "outputs": [],
      "source": [
        "# 2b)\n",
        "# Fill query here inside triple quotes and display results:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6jfNQTasOpQ",
      "metadata": {
        "id": "f6jfNQTasOpQ"
      },
      "source": [
        "[Text for your answer here]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hRfcs2LRsNvS",
      "metadata": {
        "id": "hRfcs2LRsNvS"
      },
      "outputs": [],
      "source": [
        "# 2c)\n",
        "# Your answer code with queries here:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h4rtQxxx5PRo",
      "metadata": {
        "id": "h4rtQxxx5PRo"
      },
      "source": [
        "[Text for your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e385915-367b-44dd-a3a8-beb143ad3f7d",
      "metadata": {
        "id": "4e385915-367b-44dd-a3a8-beb143ad3f7d"
      },
      "source": [
        "### Q3: Changes Over Time\n",
        "**3a.** Write a query that extracts for each tree species the change in the the number of trees in New York State from 1995 to 2015, using the dataset `'bigquery-public-data.new_york_trees.tree_census_1995'` in addition to `'bigquery-public-data.new_york_trees.tree_census_2015'`. Sort the resulting list by the absolute value of the change in the number of trees, and display the five species with the highest absolute change in the number of trees. Show the number of trees in 1995, in 2015 and the change for these species. Since the names sometimes differ between the years, use the uppercase version of `spc_latin` for each year. Then, merge the data using a `CROSS JOIN`, with the condition that the [`EDIT_DISTANCE`](https://cloud.google.com/bigquery/docs/reference/standard-sql/string_functions#edit_distance) between the names is less than or equal to 2.\n",
        "\n",
        "**3b.** Compare the percentage trees of each fall foliage color for the years 1995 and 2015 separately using the `bigquery-public-data.new_york_trees.tree_species` dataset. If a tree species is not found in the dataset, assume its foliage color is \"Unknown.\" Output a table showing for each fall foilage color the percent of trees in 1995, the percent of trees in 2015 and their difference, sorted by the aboslute value of the difference.\n",
        "Use the same criteria for `CROSS JOIN` as in 3a. ('EDIT_DISTANCE' at most 2 for the uppercase names).\n",
        "\n",
        "*Remark:* Consider each combination of colors (e.g. `Red/Bronze`) as a color of its own (different from the colors `Red` or `Bronze` in this example).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf834826-10d3-47a8-835f-c914eb4e4b68",
      "metadata": {
        "id": "bf834826-10d3-47a8-835f-c914eb4e4b68"
      },
      "source": [
        "**Answer:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2976e53c-91b4-4ace-ac0a-1684cda8cddd",
      "metadata": {
        "id": "2976e53c-91b4-4ace-ac0a-1684cda8cddd"
      },
      "outputs": [],
      "source": [
        "# 3a)\n",
        "# Your answer here:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UdoaqrSeNz7M",
      "metadata": {
        "id": "UdoaqrSeNz7M"
      },
      "source": [
        "Explanation of the results..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j9TR8gwSNuLF",
      "metadata": {
        "id": "j9TR8gwSNuLF"
      },
      "outputs": [],
      "source": [
        "# 3b)\n",
        "# Your answer here:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "npkm_MD5N2dJ",
      "metadata": {
        "id": "npkm_MD5N2dJ"
      },
      "source": [
        "Explanation of the results..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YjDeurmON5TR",
      "metadata": {
        "id": "YjDeurmON5TR"
      },
      "source": [
        "### Q4: Broken Windows Theory\n",
        "**4a.** The [Broken Windows Theory](https://en.wikipedia.org/wiki/Broken_windows_theory) suggests that poor condition of city areas may affect crime levels. In this section, we’ll look for a correlation between the proportion of dead trees (relative to total trees) and the number of `NYPD` service requests in different zip codes.\n",
        "\n",
        "Before calculating the correlation, run a query showing the different categories of the service requests to the `NYPD` and their counts (number of requests for each category) and explain whether they can be used as an imperfect measure of crime trends.\n",
        "\n",
        "Next, extract the proportion of dead trees in 2015\n",
        "for each zip code that is not `null` and has at least 100 trees, and the number of `NYPD` servise request excluding `Noise` normalized by the total number of requests in each of these zip codes for that year using SQL commands within python.\n",
        "Next, extract the results as a python object (e.g. a pandas data-frame), display a scatter-plot showing the percent of dead trees and the number of service requests across the zip codes and compute their Pearson correlation.\n",
        "\n",
        "\n",
        "Analyze the results to determine if they support the hypothesis. If they do, provide an explanation; if they don't, propose a confounding variable that might lead to inaccuracies.\n",
        "\n",
        "**Hint:** Consider using `Having` in your query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CurIBhd3O0YV",
      "metadata": {
        "id": "CurIBhd3O0YV"
      },
      "outputs": [],
      "source": [
        "# 4a)\n",
        "# Your answer here:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gOtBn9149GQF",
      "metadata": {
        "id": "gOtBn9149GQF"
      },
      "source": [
        "Explanation here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sk7jhLYHCIXs",
      "metadata": {
        "id": "sk7jhLYHCIXs"
      },
      "outputs": [],
      "source": [
        "# 4b)\n",
        "# Your answer here:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1W4fRkOk-vOr",
      "metadata": {
        "id": "1W4fRkOk-vOr"
      },
      "source": [
        "Explanation here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rbYLXPtPtROV",
      "metadata": {
        "id": "rbYLXPtPtROV"
      },
      "source": [
        "### Q5: Tree-Related Complaints  \n",
        "**5.a** Use the 311 dataset to count how many tree-related complaints each agency (for example `NYPD` or other agencies) received. Tree-related complaints are those that include the word `\"Tree\"` in the `complaint_type` field (case sensitive).\n",
        "\n",
        "**5.b** Filter the tree-related complaints to include only such complaints to the agency with the highest number of tree-related complaints you have found in (a.). Find the three most common types of these tree-related complains and display them in a table with their counts.\n",
        "Finally, create a bar chart that shows the number of the tree-related complaints of these three types for each year, where each bar is divided into three sub-bars of different colors, each indicating the tree-related complaint of each type in this year.\n",
        "\n",
        "**Hint:** consider using a window function and the pivot function in Python to simplify the plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kNofek1DxGn1",
      "metadata": {
        "id": "kNofek1DxGn1"
      },
      "outputs": [],
      "source": [
        "# 5a)\n",
        "# Your answer here:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2PL4G3o-wk3",
      "metadata": {
        "id": "d2PL4G3o-wk3"
      },
      "source": [
        "Explanation here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dVy3v62Isdq-",
      "metadata": {
        "id": "dVy3v62Isdq-"
      },
      "outputs": [],
      "source": [
        "# # 5b)\n",
        "# Your answer here:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Lm_2bosp-xJv",
      "metadata": {
        "id": "Lm_2bosp-xJv"
      },
      "source": [
        "Explanation here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-JLTtQ7KGSsI",
      "metadata": {
        "id": "-JLTtQ7KGSsI"
      },
      "source": [
        "# Unix  - 32 points\n",
        "There are 5 questions in this part. Every question is worth 6 points except question 5 that is worth 8 points. You should solve all the questions using unix commands and include them in your answer, together with additional code/analysis required and the requested results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EPHyPaqyHt1z",
      "metadata": {
        "id": "EPHyPaqyHt1z"
      },
      "source": [
        "## Q1: Simple Counting\n",
        " Copy the `review-Oregon.json.gz` file from the moriah cluster, located at\n",
        " the path `/sci/labs/orzuk/orzuk/teaching/big_data_mining_52002/midterm_2024_25` or at `/sci/home/orzuk`.\n",
        " You may work at the moriah cluster (recommended) or your personal computer.\n",
        "Extract the file's content, and use the `wc` command to display the number of lines,words, and characters in the `review-Oregon.json` file. Print the first line of the file and briefly explain what the data represents.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PmhRD0ttIID8",
      "metadata": {
        "id": "PmhRD0ttIID8"
      },
      "source": [
        "**Solutions:**\n",
        "\n",
        "**Add Question 1 Shell Commands here:**\n",
        "```\n",
        "[commands to add]\n",
        "```\n",
        "\n",
        "**Add Question 1 Shell Output here:**\n",
        "```\n",
        "[copy output to here]\n",
        "```\n",
        "\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "\n",
        "[Explain your solution here]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w2uUdkSvHwxQ",
      "metadata": {
        "id": "w2uUdkSvHwxQ"
      },
      "source": [
        "## Q2: Food and Images\n",
        "Count the number of comments that include the word `\"food\"` (**not** case sensative). Out of those comments, determine how many included at least one image.\n",
        "\n",
        "Next, perform the same query for the rest of the comments (that don't contain the word \"food\") and compare the proportions of comments with an image in the two categories. Is the difference in proportions statistically significant?\n",
        "\n",
        "Describe a statistical test and test statistic and display the results. For the last calculation of the `test statistic` and `p-value` manual computation is allowed, but all previous steps should be implemented in unix commands. Explain the Unix command you wrote in simple language.\n",
        "\n",
        "**Solutions:**\n",
        "\n",
        "**Add Question 2 Shell Commands here:**\n",
        "```\n",
        "[commands to add]\n",
        "```\n",
        "\n",
        "**Add Question 2 Shell Output here:**\n",
        "```\n",
        "[copy output to here]\n",
        "```\n",
        "\n",
        "**Add Question 2 Statistical analysis here:**\n",
        "\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "\n",
        "[Explain your solution here]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edXz-xv0Hzit",
      "metadata": {
        "id": "edXz-xv0Hzit"
      },
      "source": [
        "## Q3: Top Users\n",
        "From the people who mentioned `\"food\"` (**not** case sensative) in their comments, find the three users with the highest number of comments about `food` and display their user-ids together with the number of such comments for each one.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YejuijfxI9Zl",
      "metadata": {
        "id": "YejuijfxI9Zl"
      },
      "source": [
        "**Solutions:**\n",
        "\n",
        "**Add Question 3 Shell Commands here:**\n",
        "```\n",
        "[commands to add]\n",
        "```\n",
        "\n",
        "**Add Question 3 Shell Output here:**\n",
        "```\n",
        "[copy output to here]\n",
        "```\n",
        "\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "\n",
        "[Explain your solution here]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lRXMOhkmH3rx",
      "metadata": {
        "id": "lRXMOhkmH3rx"
      },
      "source": [
        "## Q4: Splitting\n",
        "Split the `review-Oregon.json` file into different files based on the `rating` value. Name each file as `reviews_rating_i`, where `i` represents the rating value, and count the number of comments in each of these files.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Tu2K9_99Xi4J",
      "metadata": {
        "id": "Tu2K9_99Xi4J"
      },
      "source": [
        "**Solutions:**\n",
        "\n",
        "**Add Question 4 Shell Commands here:**\n",
        "```\n",
        "[commands to add]\n",
        "```\n",
        "\n",
        "**Add Question 4 Shell Output here:**\n",
        "```\n",
        "[copy output to here]\n",
        "```\n",
        "\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "\n",
        "[Explain your solution here]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aEE0dOB6H6N7",
      "metadata": {
        "id": "aEE0dOB6H6N7"
      },
      "source": [
        "##Q5: Count Frequent Words\n",
        " Write a Python program (.py format) that takes a file name (as a string) and a natural number `k` as input. The program should read the file, extract the words from the reviews, split them into individual words, and print the top `k` most frequent words. Remove any stop words (common words) before computing the frequencies.  \n",
        "\n",
        " Next, run a shell script that runs the Python program you wrote on all the different rating files created earlier in Qu. 4 and print the top 5 most frequent words for each rating.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bMYXuvBT7NuS",
      "metadata": {
        "id": "bMYXuvBT7NuS"
      },
      "outputs": [],
      "source": [
        "# Copy code of python program here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s_xqX98fXxML",
      "metadata": {
        "id": "s_xqX98fXxML"
      },
      "source": [
        "**Solutions:**\n",
        "\n",
        "**Add Question 5 Shell Commands here:**\n",
        "```\n",
        "[commands to add]\n",
        "```\n",
        "\n",
        "**Add Question 5 Shell Output here:**\n",
        "```\n",
        "[copy output to here]\n",
        "```\n",
        "\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "\n",
        "[Explain your solution here]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wgMLZFaFnP2g",
      "metadata": {
        "id": "wgMLZFaFnP2g"
      },
      "source": [
        "# Networks - 28 points\n",
        "There are 4 questions in this part. Every question is worth 7 points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CMyl4Y4sFu60",
      "metadata": {
        "id": "CMyl4Y4sFu60"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from geopy.distance import geodesic\n",
        "import random\n",
        "from scipy.sparse import csr_matrix\n",
        "import time\n",
        "import copy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l0Lwtv82MlWd",
      "metadata": {
        "id": "l0Lwtv82MlWd"
      },
      "source": [
        "## Q1: Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pSe2zDpdnQ0o",
      "metadata": {
        "id": "pSe2zDpdnQ0o"
      },
      "source": [
        "*Download* the `web-sk-2005` network dataset available here:\n",
        "https://networkrepository.com/web.php.  \n",
        "Unzip the file and read it to python (*Note:* this can take a few minutes to upload)\n",
        "\n",
        "Display the first five rows of the dataset and explain what is shown and what the data represents.\n",
        "Count the number of nodes and edges and display them to verify that your file is correct.\n",
        "\n",
        "Finally, show two separate histograms, one for the distribution of in-degrees in the network, and one for the distribution of out-degrees. Descrbine your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EAGrjdMmbeB1",
      "metadata": {
        "id": "EAGrjdMmbeB1"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the dataset file here\n",
        "# uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i2P53q4NMtzJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2P53q4NMtzJ",
        "outputId": "76c68004-6cb2-49cd-dad6-708fcbbaa82e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Graph loaded successfully with 121422 nodes and 334419 edges.\n"
          ]
        }
      ],
      "source": [
        "def load_network_data(file_name):\n",
        "    \"\"\"\n",
        "    Write a function that loads the graph data\n",
        "    \"\"\"\n",
        "\n",
        "# Load Filepath to the web-network dataset\n",
        "graph = load_network_data(\"web-sk-2005.mtx\")\n",
        "\n",
        "# Display number of nodes and edges, and first 10 lines\n",
        "\n",
        "# Comptue and plot degree distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JbraxICSAL4Y",
      "metadata": {
        "id": "JbraxICSAL4Y"
      },
      "source": [
        "[Explanation of results here]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RW1u6ZryTnM6",
      "metadata": {
        "id": "RW1u6ZryTnM6"
      },
      "source": [
        "## Q2: Page Rank Analysis\n",
        "Run the PageRank algorithm with $\\beta=0.9$ (i.e. teleport probability of $0.1$) and with tolerance of $\\epsilon = 10^{-6}$. Plot the tolerance at each iteraction. Record the number of iterations needed for convergence.\n",
        "\n",
        "Plot the *in-degree* vs. PageRank of each node in a scatter plot. Next, plot the *out-degree* vs. PageRank of each node in a separate scatter-plot. Compute in each plot the correlation coefficient. Which one of them is more correlated to PageRank? are the results surprising? explain\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0tCRKrdhhEkC",
      "metadata": {
        "id": "0tCRKrdhhEkC"
      },
      "source": [
        "Explain in words -\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TmPibyoqFxMJ",
      "metadata": {
        "id": "TmPibyoqFxMJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Code for computing pagerank here\n",
        "\n",
        "\n",
        "# Display nodes with top pagerank score , with node ids\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HjHw5EHTn41K",
      "metadata": {
        "id": "HjHw5EHTn41K"
      },
      "source": [
        "[Explanation of results here]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oqi_oHR2p-Dy",
      "metadata": {
        "id": "oqi_oHR2p-Dy"
      },
      "outputs": [],
      "source": [
        "# Plot degrees histograms and degrees vs. pagerank scatter plots here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8_Q46wvV46nw",
      "metadata": {
        "id": "8_Q46wvV46nw"
      },
      "source": [
        "[Explanation of results here]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jFUofGrUaBaz",
      "metadata": {
        "id": "jFUofGrUaBaz"
      },
      "source": [
        "## Q3: PageRank Analysis of Spammers\n",
        "\n",
        "Suppose that a spammer wants to inflater the PageRank of some pages. Choose random 100 pages and add edges between every pair of them (in both directions). Comptue the resulting PageRank for the new network. Next make two scatter plots:\n",
        "(i.) The page-rank of each page before and after the change to the network by the spammers\n",
        "(ii.) The in-degree vs. the pagerank after the change to the network by the spammer\n",
        "\n",
        "Explain the changes observed after spamming.\n",
        "\n",
        "Finally, repeat all steps above (adding spam edges, computing modified page-rank, plotting and explaining the results) for a different spammer strategy: choosing two distinct random sets of 100 nodes, and adding all possible links form the first set to the second set.\n",
        "Which spamming strategy is more effective?\n",
        "\n",
        "*Note:* Highlight in different colors the different sets of nodes in your scatter plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qGkiLLhtazLd",
      "metadata": {
        "id": "qGkiLLhtazLd"
      },
      "outputs": [],
      "source": [
        "# Code for adding edges here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7YJ5PzDa5OIa",
      "metadata": {
        "id": "7YJ5PzDa5OIa"
      },
      "source": [
        "[Explanation  here]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exReS8t-d4T5",
      "metadata": {
        "id": "exReS8t-d4T5"
      },
      "outputs": [],
      "source": [
        "# Code for computing and displaying updated pagerank and degrees here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1FGQ9IeeB7K4",
      "metadata": {
        "id": "1FGQ9IeeB7K4"
      },
      "source": [
        "[Explanation of results here]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xOi47Rn9UGhP",
      "metadata": {
        "id": "xOi47Rn9UGhP"
      },
      "source": [
        "## Q4: PageRank Computational Complexity\n",
        "\n",
        "Suppose that $A$ is an $n \\times n$ matrix with distinct real eigenvalues and we implement the power method to find the leading eigenvector of $A$.\n",
        "\n",
        "- What is the computational complexity of each iteration of the Power method as a function of $n$ for a general (dense) matrix $A$ as a function of $n$ (use Big-O notation)?\n",
        "\n",
        "- Suppose that we know that $A = B+C$ where $B$ is a sparse matrix with $s$ non-zero values, and $C$ is a low-rank matrix with $rank(C)=k$. Write the algorithm of the Power method using $C$ and $B$ as input and utilizing their structure. What is the computational complexity as a function of $n$, $k$ and $s$? (use Big-O notation). You should represent $B$ and $C$ in a way that will minimize memory usage and computations.\n",
        "\n",
        "- Suppose that we know that the convergence rate of the Power method is quadratic. That is, after $t$ iterations the $L_2$ distance between the pagerank vector $r^t$ computed after $t$ iterations and the true pagerank vector $r$ satisfies $||r^t - r||_2 \\leq \\frac{a}{t^2}$ for some constant $a > 0$. \\\\\n",
        "We run the algorithm until reaching tolearance $\\epsilon$ , i.e. our algorithm should return a vector $r'$ such that $||r'-r||_2 \\leq \\epsilon$. What will be the computational complexity of the entire algorithm for the above two cases as a function of $n,k,s$ and $\\epsilon$? (you may assume that the constant $a$ is known)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pPeQO3en5dNC",
      "metadata": {
        "id": "pPeQO3en5dNC"
      },
      "source": [
        "[Solution text with Explanations of Computational Complexity of the different algorithms here]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
