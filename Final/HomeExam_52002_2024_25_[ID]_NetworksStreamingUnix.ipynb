{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c71a501a-e69f-4db7-871f-905e9861fe4a",
      "metadata": {
        "id": "c71a501a-e69f-4db7-871f-905e9861fe4a"
      },
      "source": [
        "# Home Exam 52002 - 2024-2025"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yi4B9L4mjR8b",
      "metadata": {
        "id": "yi4B9L4mjR8b"
      },
      "source": [
        "#**Instructions**\n",
        "* **Fill your ID Here:** [replace the bracketed text with your ID number]\n",
        "* Work on the assignment and submit your solution *individually*. <br>\n",
        "No sharing of information on the assignment is allowed between students.\n",
        "\n",
        "\n",
        "* **Format:** Fill code, text explanations and output (figures, tables ..) in the designated places. <br>For some questions, the code you fill should run in this .ipynb notebook and generate the output automatically after running (e.g. in `google colab`). <br>For the Unix part you will need to run commands in other environments (Ubuntu) - in this case, just copy the commands and the relevant outputs in the designated text blocks.\n",
        "Rename the solution file to 'HomeExam_52002_2045_25_[ID].ipynb' where [ID] should be replaced by your ID number.\n",
        "* Submit your filled solution by Febuary 28th 23:59 your solution on moodle.\n",
        "\n",
        "\n",
        "* **Data:** Some of the questions requires analyzing and manipulating data files. All the files required for the exam are located in the directory:\n",
        "'/sci/home/orzuk/FinalExamBigData'\n",
        "in Moriah.\n",
        "You may copy them to your working directory in Moriah, to your personal computer or any other computing environment you use. You may need to unzip the files before using them.\n",
        "\n",
        "\n",
        "* **Grading:**\n",
        "*  There are overall $11$ questions in this home exam. Each question is worth $9$ points to your total grade. One additional point will be given for submitting  files with correct formats and file names. * **Note:** Points from your grade may be deducted for submitting wrong/missing parts of files OR if not submitting the complete generated/complied output.\n",
        "* **Note:** Some parts of the code may take a long time  to run.\n",
        " Be patient. However, don't leave everything to run at the last minute but prepare in advance so that your entire solution runs and finishes on time.\n",
        "* **Note**: Solutions with naive or inefficient implementations may reduce the score\n",
        "\n",
        "###Submission Guidelines:\n",
        "\n",
        "\n",
        "By the end of the exercise, please submit the following **four** files:\n",
        "\n",
        "\n",
        "1. **Networks, Streaming, Unix, and Batch Task Processing:**  \n",
        "   - Provide your solutions in both `.ipynb` (Jupyter Notebook) and `.html` formats. Submit after running all parts of the `.ipynb` notebook except the unix part , check that the outputs of each question were created and saved. For the unix part, copy the code and results manually to the `.ipynb` notebook.\n",
        "\n",
        "2. **Spark Section:**  \n",
        "   - Submit the fully executed Jupyter Notebook (`.ipynb`) with all expected outputs, after running it in the Databricks environment.\n",
        "   - Include an `.html` export of the executed notebook displaying the outputs.  \n",
        "\n",
        "\n",
        "Ensure that all submitted files are clearly labeled and display the required outputs where applicable.\n",
        "\n",
        "* **Good luck!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-JLTtQ7KGSsI",
      "metadata": {
        "id": "-JLTtQ7KGSsI"
      },
      "source": [
        "# Part 1: Unix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-02tm8Q9H1xz",
      "metadata": {
        "id": "-02tm8Q9H1xz"
      },
      "source": [
        "## Q1. Preprocessing using Unix\n",
        "The file `network-review-Oregon.json` contains user reviews of different buisnesses (it is a sample from a full `review-Oregon.json` file).\n",
        "\n",
        "a. Use Unix commands to generate a new file called `bipartite_network.txt'\n",
        "containing a table from the file `network-review-Oregon.json`\n",
        "The table should contain only the next columns:\n",
        "`user_id,gmap_id_from,rating` separated by commmas\n",
        "\n",
        "Finally, show all the rows in which the user-id is `100000837087364476756`\n",
        "\n",
        "b. Use Unix commands and the file from (a.) to generate a new\n",
        "`network-table.txt` containing one row for each pair of buisnesses (`gmap_id`) that were reviewed by the same user (`user_id`).  \n",
        "\n",
        "The table should contain only the next columns:\n",
        "`gmap_id_from,user_id,rating_from,gmap_id_to,rating_to` separated by commmas\n",
        "\n",
        "You can split your process into multiple steps, creating intermediate CSV/TXT files and then merging them.\n",
        "\n",
        "Finally, show all the rows in which the user-id is `100000837087364476756`\n",
        "\n",
        "**Note:** If two different users rate the same two buisnesses, there should be separate rows for the ratings. In addition, the pair of buisnesses should appear twice in the two possible orders.\n",
        "For example if user1 rated the two buisnesses as 3,4 and user2 rated them as 4,2, the following lines should be in your output file:\n",
        "gmap_id1, user1, 3, gmap_id2, 4\n",
        "gmap_id2, user1, 4, gmap_id1, 3\n",
        "gmap_id1, user2, 4, gmap_id2, 2\n",
        "gmap_id2, user2, 2, gmap_id1, 4\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Rsu2wqohXVFv",
      "metadata": {
        "id": "Rsu2wqohXVFv"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "**Question 1 Shell Commands ($):**\n",
        "```\n",
        "Write you code here\n",
        "\n",
        "```\n",
        "\n",
        "**Question 1 Shell Output ($):**\n",
        "```\n",
        "Write your output here\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SMKMrB9_V-J7",
      "metadata": {
        "id": "SMKMrB9_V-J7"
      },
      "source": [
        "## Q2. Batch Task Processing- Moriah\n",
        "\n",
        "We want to calcaulte the total number of reviews and the average rating for each `gmap_id` of the full review file (`review-Oregon.json`)\n",
        "\n",
        "Implement the following pipeline for this task:\n",
        "1. Split the input file into five files, one for each rating from 1 to 5 (e.g., `rating_i.txt`) using unix.\n",
        "2. For each file, submit a job with python script that calculates how many times each `gmap_id` appears. Save the results in a CSV file (e.g. `rating_i_counts.txt`.)\n",
        "3. Run a final Python script to:\n",
        " - Read all the CSV files.\n",
        " - Combine the results.\n",
        " - Calculate the average rating of `gmap_id` and the total number of reviews.\n",
        " - Use the file `meta-Oregon.json` to map `gmap_id` to the buisness name\n",
        "\n",
        "Print the top three `gmap_id` values sorted by rating, with ties broken by sorting by the number of reviews (both in descending order).\n",
        "The output table should be with the next columns:\n",
        " `gmap_id`,`name`,`avg_rating`,`total_reviews`.\n",
        "\n",
        "\n",
        "**Note: Steps 2-3 should be as pipline in single bash file**\n",
        "\n",
        "**Hint:** Use job dependencies because the tasks need to run in order.\n",
        "You can use [moriah wiki](https://wiki.rcs.huji.ac.il/hurcs/guides/slurm) to learn more.\n",
        "\n",
        "**Guidence:** Every user has limited compute power. So first write your script on sample of the data and run on the local machine and only when you think that you code is good run on Moriah.\n",
        "\n",
        "After completing and running the pipelinee, copy the unix script, python code and the results table in the next chunks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "znd2x5g9xYEu",
      "metadata": {
        "id": "znd2x5g9xYEu"
      },
      "source": [
        "Unix code to devide by rating<br>\n",
        "**Here**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dQR0e4uFjL1b",
      "metadata": {
        "id": "dQR0e4uFjL1b"
      },
      "source": [
        "Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-muIbXnSIMef",
      "metadata": {
        "id": "-muIbXnSIMef"
      },
      "outputs": [],
      "source": [
        "## file name =  `First.py`\n",
        "\n",
        "## code starts here\n",
        "import pandas as pd\n",
        "import re\n",
        "import sys\n",
        "\n",
        "def count_gamp(your_input)->pd.DataFrame:\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    rating_path = sys.argv[1]  # Get file path from command line\n",
        "    results = count_gamp(rating_path)\n",
        "    #....\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0BOM3MfDMz9W",
      "metadata": {
        "id": "0BOM3MfDMz9W"
      },
      "outputs": [],
      "source": [
        "#file name =  `Second.py`\n",
        "\n",
        "## code starts here\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "\n",
        "\n",
        "def calc_avg_mean(your_input) -> pd.DataFrame:\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # ...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zCRAETQ6jOsT",
      "metadata": {
        "id": "zCRAETQ6jOsT"
      },
      "source": [
        "Bash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ObZfYzJTaC2",
      "metadata": {
        "id": "2ObZfYzJTaC2"
      },
      "outputs": [],
      "source": [
        "## file name =  `main_bash.py`\n",
        "\n",
        "## code starts here\n",
        "#!/bin/bash\n",
        "# Code in bash ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "P2vC8lWdo-e5",
      "metadata": {
        "id": "P2vC8lWdo-e5"
      },
      "source": [
        "Output table for three top `gmap_ids`:\n",
        "\n",
        "**Here**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wDDxzqKMM2vn",
      "metadata": {
        "id": "wDDxzqKMM2vn"
      },
      "outputs": [],
      "source": [
        "# unix code - same as the midterm code.."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90ACWpnQx2UD",
      "metadata": {
        "id": "90ACWpnQx2UD"
      },
      "source": [
        "py file to calcaulte the avg_mean and number of revires"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WAhUAOUy_ICy",
      "metadata": {
        "id": "WAhUAOUy_ICy"
      },
      "source": [
        "# Part 2 : Streaming Algorithms\n",
        "\n",
        "## Q1. Streaming Sampling Algorithm\n",
        "- Write python function that reads the `review-Oregon.json` file **line-by-line**, i,e, **one line at a time** (see code template below).  Your code should implement  \n",
        "online sampling of 1000 random users and all of their ratings. That is, you should initialize and update a data structue such that\n",
        "after each value of $n$ lines that were processed that correspond to $k \\leq n$ distinct users, it should hold that your data structure stores the identity of $min(k, 1000)$ users chosen uniformly at random from the first $k$ users (without replacement), and will also store **all** lines corresponding to these users\n",
        "\n",
        "- After finishing to process all lines in the file, compute 'ave_rating' for all buisnesses using this sample of $1000$ users, and make a scatter plot of this `ave_rating` vs. the `ave_rating` from Unix Q2 using all users. Decsribe the results\n",
        "\n",
        "**Notes:** You should never store the entire file in memory. After reading each line, if you decide to not include the corresponding user in your sample you should throw it away and never use it again.\n",
        "Exclude from the scatter plot buisnesses that were not reviewed by any of the 1000 users in your sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jLIP9BvFlgQT",
      "metadata": {
        "id": "jLIP9BvFlgQT"
      },
      "outputs": [],
      "source": [
        "# Your initialization code here\n",
        "\n",
        "# Open the file in read mode and process it line-by-line.\n",
        "with open(\"review-Oregon.json\", \"r\") as file:\n",
        "    for line in file:\n",
        "        # Add code here to process line and update your sample"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gdH-T-fhljbl",
      "metadata": {
        "id": "gdH-T-fhljbl"
      },
      "source": [
        "# Part 3: Networks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lMc8QAh6h5am",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMc8QAh6h5am",
        "outputId": "8829a27d-7058-4315-ed87-b7010bc6ee93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.10/dist-packages (0.16)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from python-louvain) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from python-louvain) (1.26.4)\n",
            "Requirement already satisfied: folium in /usr/local/lib/python3.10/dist-packages (0.19.4)\n",
            "Requirement already satisfied: branca>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from folium) (0.8.1)\n",
            "Requirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from folium) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from folium) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from folium) (2.32.3)\n",
            "Requirement already satisfied: xyzservices in /usr/local/lib/python3.10/dist-packages (from folium) (2024.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.9->folium) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->folium) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->folium) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->folium) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->folium) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "! pip install python-louvain\n",
        "! pip install folium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CMyl4Y4sFu60",
      "metadata": {
        "id": "CMyl4Y4sFu60"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import community as community_louvain\n",
        "from geopy.distance import geodesic\n",
        "import folium\n",
        "import random\n",
        "from branca.colormap import LinearColormap"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l0Lwtv82MlWd",
      "metadata": {
        "id": "l0Lwtv82MlWd"
      },
      "source": [
        "## Q1. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pSe2zDpdnQ0o",
      "metadata": {
        "id": "pSe2zDpdnQ0o"
      },
      "source": [
        "a. Read the data file `network-table.txt` you created in the the previous Unix question.  \n",
        "In addition, read the file *meta-Oregon.json*, which contains additional information about each `gmap_id`, such as category, website, and more.  \n",
        "Display the first five rows of each dataset and explain what is shown and what does the data represents.\n",
        "\n",
        "**Note:** If you failed to create the correct `network-table.txt` file in the unix part, you can use for this question the file we supply.\n",
        "\n",
        "b. Read the file `network-table.txt` and plot a histogram showing the number of reviews by each user (`user_id`).\n",
        "Next, plot a histogram showing the number of unique users reviewing each buisness (`gmap_id`).\n",
        "\n",
        "c.\n",
        "  - Display the distribution of buisness categories using the `meta-Oregon` file. For each buisness having multiple categories use only the first `category`. Show only the top 30 categories having the largest number of buisnesses. Highlight all the restaurant cateories  in a different color.  \n",
        "  - Choose 4 of the top 30 categories and show for each one of them the distribution of `avg_rating` (from the `meta-Oregon` file) for this cateroy.\n",
        "\n",
        "d. Finally, filter the  the `meta-Oregon` file to include only buisnesses with more than 100 reviews. Use `Folium` to create a map showing the buisnesses with more than 100 reviews as circles, color them by the `avg_rating` between red to green and make their size proportional to the number of reviews (use `radius` = `num_reviews` / 1000).\n",
        "Using all of these, describe the data and explain its meaning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i2P53q4NMtzJ",
      "metadata": {
        "id": "i2P53q4NMtzJ"
      },
      "source": [
        "**Solution**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xRfoLHX7O_ye",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRfoLHX7O_ye",
        "outputId": "a6c2ded4-9008-4a3f-9378-85f84ab6273e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gXajitWYjy1X",
      "metadata": {
        "id": "gXajitWYjy1X"
      },
      "outputs": [],
      "source": [
        "# Load the data and show the tables ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P6Duc7msj8yj",
      "metadata": {
        "id": "P6Duc7msj8yj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "Ohve7JtzUMDh",
      "metadata": {
        "id": "Ohve7JtzUMDh"
      },
      "source": [
        "explnation..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11ISAHLrkAW0",
      "metadata": {
        "id": "11ISAHLrkAW0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "-WMval9NkGoN",
      "metadata": {
        "id": "-WMval9NkGoN"
      },
      "source": [
        "explnation..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RW1u6ZryTnM6",
      "metadata": {
        "id": "RW1u6ZryTnM6"
      },
      "source": [
        "## Q2. Community Detection - The Louvain Algorithm\n",
        "- Explain the Louvain algorithm in words, provide an example with at least 6 nodes, display it on a plot, and mark the communities that are formed in the example according to the algorithm (you can use the `community_louvain` implementation).\n",
        "\n",
        "-  Make one change in the divison to communities (move at least one node from one community to another) and show that the resulting division is sub-optimal by comparing the cost function of the two communities. Write explicitly the formulas that you use."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dGD-xVHnlLfY",
      "metadata": {
        "id": "dGD-xVHnlLfY"
      },
      "source": [
        "**Solution**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0tCRKrdhhEkC",
      "metadata": {
        "id": "0tCRKrdhhEkC"
      },
      "source": [
        "Explaining in words:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "na0_SHP1kXM8",
      "metadata": {
        "id": "na0_SHP1kXM8"
      },
      "outputs": [],
      "source": [
        "# Code that plot the example of 6 nodes devided into communites"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SagiEuNtkmd7",
      "metadata": {
        "id": "SagiEuNtkmd7"
      },
      "source": [
        "Suggest change and show the impact using the formulas..  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ADImk8l6iamc",
      "metadata": {
        "id": "ADImk8l6iamc"
      },
      "source": [
        "## Q3. Network Preliminary Analysis\n",
        "Represent the data file 'network_data.txt' you've loaded as an **undirected** graph using the `gmap_id_from` and `gmap_id_to` fields:\n",
        "Make sure that each undorederd (`gmap_id_from` and `gmap_id_to`) appears only once, and remove self loops.\n",
        "\n",
        "Next, analyze the network:\n",
        "- First, print the number nodes and edges in the graph.\n",
        "- Then, plot the degree distribution and explain what the plot reveals.\n",
        "- Finally, visualize the entire graph and describe what you observe. Use the default layout of `networkx` draw function.\n",
        "\n",
        "**Note: the plot might take a while to run..**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8HyGo4cplM7-",
      "metadata": {
        "id": "8HyGo4cplM7-"
      },
      "source": [
        "**Solution**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SwGcA5BdTTcw",
      "metadata": {
        "id": "SwGcA5BdTTcw"
      },
      "outputs": [],
      "source": [
        "G = nx.Graph()\n",
        "\n",
        "for _, row in network_df.iterrows():\n",
        "    G.add_edge(\n",
        "        # ...\n",
        "       )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YYHXxMtdXgfE",
      "metadata": {
        "id": "YYHXxMtdXgfE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "xOi47Rn9UGhP",
      "metadata": {
        "id": "xOi47Rn9UGhP"
      },
      "source": [
        "## Q4. Network Community Analysis\n",
        "\n",
        "- We want to focus on buisnesses with many reviews. Keep only the nodes that have at least 15 edges. Afterward, remove nodes that are not connected to the central part of the network, i.e. the largest connected component.  \n",
        "\n",
        "You can use the following code to help:  \n",
        "```\n",
        "largest_cc = max(nx.connected_components(subgraph), key=len)  \n",
        "main_component = subgraph.subgraph(largest_cc)  \n",
        "```\n",
        "\n",
        "- Next, run the **Louvain algorithm** to divide the buisnesses in the main connected component into communities. Plot the buisnesses colored by their coomunity and describe the results in detail.\n",
        "\n",
        "- Finally, we want to know whether communities reflect different categories.\n",
        "— For each community compute the fraction of buisnesses form this community in each of the 30 top cateories from Q1, plus a 31st cateorgy called 'other' for all buisnesses in a different category\n",
        "- Plot a heatmap showing the community-by-category fractions.\n",
        "Do you see a relationship between the communities and categories? derive a staitstical test testing the null hypothesis of no such relationship, and report your test results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DpSIoFnjlP76",
      "metadata": {
        "id": "DpSIoFnjlP76"
      },
      "source": [
        "**Solution**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93uY2CQ0fCWE",
      "metadata": {
        "id": "93uY2CQ0fCWE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "jFUofGrUaBaz",
      "metadata": {
        "id": "jFUofGrUaBaz"
      },
      "source": [
        "## Q5. Geographic Community Analysis\n",
        " In this question we will explore whether there is any geographical significance to the communities that were formed.\n",
        "-  Choose the **five largest communities**. For each one of them, calculate the average latitude and longitude $\\text{Lon, Lat}$ of all locations within it.\n",
        "\n",
        "- Then, compute the Empirical Cumulative Distribution Function (**ECDF**) of the distances from each location to the center of buisnesses in this community. Plot in addition the **ECDF** of the distance to the center for all locations in the entire dataset.\n",
        "What do the results show? Do you observe geographical clustering of the communities?  \n",
        "\n",
        "- Next, plot the points of the buisnesses in the largest five communities on a map using the `folium` library, with each community displayed in a different color. What do you observe? Explain in a couple of sentences the results and why they might occur."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uyMh01filWqr",
      "metadata": {
        "id": "uyMh01filWqr"
      },
      "source": [
        "**Solution**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XnHXdgRVldeh",
      "metadata": {
        "id": "XnHXdgRVldeh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
