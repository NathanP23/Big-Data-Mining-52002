{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c71a501a-e69f-4db7-871f-905e9861fe4a",
      "metadata": {
        "id": "c71a501a-e69f-4db7-871f-905e9861fe4a"
      },
      "source": [
        "# Home Exam 52002 - 2024-2025"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yi4B9L4mjR8b",
      "metadata": {
        "id": "yi4B9L4mjR8b"
      },
      "source": [
        "#**Instructions**\n",
        "* **Fill your ID Here:** [replace the bracketed text with your ID number]\n",
        "* Work on the assignment and submit your solution *individually*. <br>\n",
        "No sharing of information on the assignment is allowed between students.\n",
        "\n",
        "\n",
        "* **Format:** Fill code, text explanations and output (figures, tables ..) in the designated places. <br>For some questions, the code you fill should run in this .ipynb notebook and generate the output automatically after running (e.g. in `google colab`). <br>For the Unix part you will need to run commands in other environments (Ubuntu) - in this case, just copy the commands and the relevant outputs in the designated text blocks.\n",
        "Rename the solution file to 'HomeExam_52002_2045_25_[ID].ipynb' where [ID] should be replaced by your ID number.\n",
        "* Submit your filled solution by Febuary 28th 23:59 your solution on moodle.\n",
        "\n",
        "\n",
        "* **Data:** Some of the questions requires analyzing and manipulating data files. All the files required for the exam are located in the directory:\n",
        "'/sci/home/orzuk/FinalExamBigData'\n",
        "in Moriah.\n",
        "You may copy them to your working directory in Moriah, to your personal computer or any other computing environment you use. You may need to unzip the files before using them.\n",
        "\n",
        "\n",
        "* **Grading:**\n",
        "*  There are overall $11$ questions in this home exam. Each question is worth $9$ points to your total grade. One additional point will be given for submitting  files with correct formats and file names. * **Note:** Points from your grade may be deducted for submitting wrong/missing parts of files OR if not submitting the complete generated/complied output.\n",
        "* **Note:** Some parts of the code may take a long time  to run.\n",
        " Be patient. However, don't leave everything to run at the last minute but prepare in advance so that your entire solution runs and finishes on time.\n",
        "* **Note**: Solutions with naive or inefficient implementations may reduce the score\n",
        "\n",
        "###Submission Guidelines:\n",
        "\n",
        "\n",
        "By the end of the exercise, please submit the following **four** files:\n",
        "\n",
        "\n",
        "1. **Networks, Streaming, Unix, and Batch Task Processing:**  \n",
        "   - Provide your solutions in both `.ipynb` (Jupyter Notebook) and `.html` formats. Submit after running all parts of the `.ipynb` notebook except the unix part , check that the outputs of each question were created and saved. For the unix part, copy the code and results manually to the `.ipynb` notebook.\n",
        "\n",
        "2. **Spark Section:**  \n",
        "   - Submit the fully executed Jupyter Notebook (`.ipynb`) with all expected outputs, after running it in the Databricks environment.\n",
        "   - Include an `.html` export of the executed notebook displaying the outputs.  \n",
        "\n",
        "\n",
        "Ensure that all submitted files are clearly labeled and display the required outputs where applicable.\n",
        "\n",
        "* **Good luck!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-JLTtQ7KGSsI",
      "metadata": {
        "id": "-JLTtQ7KGSsI"
      },
      "source": [
        "# Part 1: Unix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-02tm8Q9H1xz",
      "metadata": {
        "id": "-02tm8Q9H1xz"
      },
      "source": [
        "## Q1. Preprocessing using Unix\n",
        "The file `network-review-Oregon.json` contains user reviews of different buisnesses (it is a sample from a full `review-Oregon.json` file).\n",
        "\n",
        "a. Use Unix commands to generate a new file called `bipartite_network.txt'\n",
        "containing a table from the file `network-review-Oregon.json`\n",
        "The table should contain only the next columns:\n",
        "`user_id,gmap_id_from,rating` separated by commmas\n",
        "\n",
        "Finally, show all the rows in which the user-id is `100000837087364476756`\n",
        "\n",
        "b. Use Unix commands and the file from (a.) to generate a new\n",
        "`network-table.txt` containing one row for each pair of buisnesses (`gmap_id`) that were reviewed by the same user (`user_id`).  \n",
        "\n",
        "The table should contain only the next columns:\n",
        "`gmap_id_from,user_id,rating_from,gmap_id_to,rating_to` separated by commmas\n",
        "\n",
        "You can split your process into multiple steps, creating intermediate CSV/TXT files and then merging them.\n",
        "\n",
        "Finally, show all the rows in which the user-id is `100000837087364476756`\n",
        "\n",
        "**Note:** If two different users rate the same two buisnesses, there should be separate rows for the ratings. In addition, the pair of buisnesses should appear twice in the two possible orders.\n",
        "For example if user1 rated the two buisnesses as 3,4 and user2 rated them as 4,2, the following lines should be in your output file:\n",
        "gmap_id1, user1, 3, gmap_id2, 4\n",
        "gmap_id2, user1, 4, gmap_id1, 3\n",
        "gmap_id1, user2, 4, gmap_id2, 2\n",
        "gmap_id2, user2, 2, gmap_id1, 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Rsu2wqohXVFv",
      "metadata": {
        "id": "Rsu2wqohXVFv",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "**Question 1 Shell Commands ($):**<br>\n",
        "\n",
        "a.\n",
        "\n",
        "```\n",
        "zcat /sci/home/orzuk/BigDataMiningExam/network-review-Oregon.json.gz | jq -r '[.user_id, .gmap_id, .rating] | @csv' > bipartite_network.txt\n",
        "grep '\"100000837087364476756\",' bipartite_network.txt\n",
        "```\n",
        "\n",
        "b.\n",
        "```\n",
        "awk -F, '\n",
        "{\n",
        "    key = $1 \"_\" $2  # Create a unique key: \"user_id_gmap_id\"\n",
        "    ratings[key] = $3\n",
        "    user_reviews[$1] = user_reviews[$1] \",\" $2  # Append business IDs for this user\n",
        "} \n",
        "END {\n",
        "    for (user in user_reviews) {\n",
        "        split(user_reviews[user], businesses, \",\")  # Convert list to array\n",
        "        n = length(businesses)\n",
        "        for (i = 2; i < n; i++) {  # Start at index 2 to skip leading comma\n",
        "            for (j = i + 1; j <= n; j++) {  # Ensure i ≠ j and avoid duplicates\n",
        "                key1 = user \"_\" businesses[i]\n",
        "                key2 = user \"_\" businesses[j]\n",
        "\n",
        "                # Print the pair in both orders\n",
        "                print businesses[i] \",\" user \",\" ratings[key1] \",\" businesses[j] \",\" ratings[key2]\n",
        "                print businesses[j] \",\" user \",\" ratings[key2] \",\" businesses[i] \",\" ratings[key1]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}' bipartite_network.txt > network-table.txt\n",
        "grep '\"100000837087364476756\",' network-table.txt\n",
        "```\n",
        "\n",
        "**Question 1 Shell Output ($):**<br>\n",
        "a.\n",
        "```\n",
        "\"100000837087364476756\",\"0x54950a755cc35d8d:0x92d6d400144b2141\",5\n",
        "\"100000837087364476756\",\"0x54c17ff7bcaa7c31:0x2970a958143cc922\",4\n",
        "```\n",
        "\n",
        "b.\n",
        "```\n",
        "\"0x54950a755cc35d8d:0x92d6d400144b2141\",\"100000837087364476756\",5,\"0x54c17ff7bcaa7c31:0x2970a958143cc922\",4\n",
        "\"0x54c17ff7bcaa7c31:0x2970a958143cc922\",\"100000837087364476756\",4,\"0x54950a755cc35d8d:0x92d6d400144b2141\",5\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SMKMrB9_V-J7",
      "metadata": {
        "id": "SMKMrB9_V-J7"
      },
      "source": [
        "## Q2. Batch Task Processing- Moriah\n",
        "\n",
        "We want to calcaulte the total number of reviews and the average rating for each `gmap_id` of the full review file (`review-Oregon.json`)\n",
        "\n",
        "Implement the following pipeline for this task:\n",
        "1. Split the input file into five files, one for each rating from 1 to 5 (e.g., `rating_i.txt`) using unix.\n",
        "2. For each file, submit a job with python script that calculates how many times each `gmap_id` appears. Save the results in a CSV file (e.g. `rating_i_counts.txt`.)\n",
        "3. Run a final Python script to:\n",
        " - Read all the CSV files.\n",
        " - Combine the results.\n",
        " - Calculate the average rating of `gmap_id` and the total number of reviews.\n",
        " - Use the file `meta-Oregon.json` to map `gmap_id` to the buisness name\n",
        "\n",
        "Print the top three `gmap_id` values sorted by rating, with ties broken by sorting by the number of reviews (both in descending order).\n",
        "The output table should be with the next columns:\n",
        " `gmap_id`,`name`,`avg_rating`,`total_reviews`.\n",
        "\n",
        "\n",
        "**Note: Steps 2-3 should be as pipline in single bash file**\n",
        "\n",
        "**Hint:** Use job dependencies because the tasks need to run in order.\n",
        "You can use [moriah wiki](https://wiki.rcs.huji.ac.il/hurcs/guides/slurm) to learn more.\n",
        "\n",
        "**Guidence:** Every user has limited compute power. So first write your script on sample of the data and run on the local machine and only when you think that you code is good run on Moriah.\n",
        "\n",
        "After completing and running the pipelinee, copy the unix script, python code and the results table in the next chunks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dQR0e4uFjL1b",
      "metadata": {
        "id": "dQR0e4uFjL1b"
      },
      "source": [
        "Python files (First.py and Second.py):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-muIbXnSIMef",
      "metadata": {
        "id": "-muIbXnSIMef"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "def count_gmap_ids(input_file, output_file):\n",
        "    try:\n",
        "        # Load CSV file (ensuring proper column names)\n",
        "        df = pd.read_csv(input_file)\n",
        "        # Ensure 'gmap_id' column exists\n",
        "        if \"gmap_id\" not in df.columns:\n",
        "            print(f\"❌ Error: 'gmap_id' column not found in {input_file}\")\n",
        "            return\n",
        "        # Count occurrences of gmap_id\n",
        "        gmap_counts = df[\"gmap_id\"].value_counts().reset_index()\n",
        "        gmap_counts.columns = [\"gmap_id\", \"count\"]\n",
        "        # Save results to a CSV-formatted TXT file\n",
        "        gmap_counts.to_csv(output_file, index=False)\n",
        "        print(f\"✅ Completed processing: {input_file} -> {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing {input_file}: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if len(sys.argv) < 3:\n",
        "        print(\"Usage: python3 count_gmap.py <input_file> <output_file>\")\n",
        "        sys.exit(1)\n",
        "    input_file = sys.argv[1]\n",
        "    output_file = sys.argv[2]\n",
        "    count_gmap_ids(input_file, output_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0BOM3MfDMz9W",
      "metadata": {
        "id": "0BOM3MfDMz9W"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import sys\n",
        "import os\n",
        "\n",
        "def aggregate_reviews(data_dir, meta_file, output_file):\n",
        "    try:\n",
        "        # Load metadata to map gmap_id -> business name\n",
        "        gmap_to_name = {}\n",
        "        with open(meta_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    item = json.loads(line.strip())  # Read each line as a JSON object\n",
        "                    gmap_to_name[item[\"gmap_id\"]] = item[\"name\"]\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"⚠️ Skipping malformed JSON line: {line.strip()}\")\n",
        "\n",
        "        # Initialize dataframe\n",
        "        all_data = pd.DataFrame(columns=[\"gmap_id\", \"count\", \"rating\"])\n",
        "\n",
        "        # Process each rating_i_counts.txt file\n",
        "        for i in range(1, 6):\n",
        "            rating_file = os.path.join(data_dir, f\"rating_{i}_counts.txt\")\n",
        "            if os.path.exists(rating_file) and os.path.getsize(rating_file) > 0:\n",
        "                df = pd.read_csv(rating_file)\n",
        "                df[\"rating\"] = i  # Assign the rating value to each row\n",
        "                all_data = pd.concat([all_data, df], ignore_index=True)\n",
        "\n",
        "        # Aggregate: Calculate total reviews & average rating per gmap_id\n",
        "        grouped = all_data.groupby(\"gmap_id\").agg(\n",
        "            total_reviews=pd.NamedAgg(column=\"count\", aggfunc=\"sum\"),\n",
        "            avg_rating=pd.NamedAgg(column=\"rating\", aggfunc=\"mean\")\n",
        "        ).reset_index()\n",
        "\n",
        "        # Map business names\n",
        "        grouped[\"name\"] = grouped[\"gmap_id\"].map(gmap_to_name)\n",
        "\n",
        "        # Sort: First by avg_rating (desc), then by total_reviews (desc)\n",
        "        grouped = grouped.sort_values(by=[\"avg_rating\", \"total_reviews\"], ascending=[False, False])\n",
        "\n",
        "        # Save results\n",
        "        grouped.to_csv(output_file, index=False)\n",
        "        print(f\"✅ Final results saved to: {output_file}\")\n",
        "\n",
        "        # Print top 3 businesses\n",
        "        print(\"\\n🏆 Top 3 Businesses:\")\n",
        "        print(grouped.head(3).to_string(index=False))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error in aggregation: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if len(sys.argv) < 4:\n",
        "        print(\"Usage: python3 Second.py <data_directory> <meta_file> <output_file>\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    data_dir = sys.argv[1]\n",
        "    meta_file = sys.argv[2]\n",
        "    output_file = sys.argv[3]\n",
        "\n",
        "    aggregate_reviews(data_dir, meta_file, output_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zCRAETQ6jOsT",
      "metadata": {
        "id": "zCRAETQ6jOsT"
      },
      "source": [
        "Bash file (main_batch.sh):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f72895f2",
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "#!/bin/bash\n",
        "#SBATCH --job-name=BigDataMiningP1Q2_bigdata_pipeline\n",
        "#SBATCH --output=bigdata_pipeline.out\n",
        "#SBATCH --error=bigdata_pipeline.err\n",
        "#SBATCH --time=2:00:00\n",
        "#SBATCH --mem=16G\n",
        "#SBATCH --cpus-per-task=4\n",
        "\n",
        "DATA_DIR=\"Data\"\n",
        "mkdir -p \"$DATA_DIR\"\n",
        "OrZukData=\"/sci/home/orzuk/BigDataMiningExam\"\n",
        "REVIEW_FILE=\"$OrZukData/review-Oregon.json.gz\"\n",
        "META_FILE=\"$OrZukData/meta-Oregon.json.gz\"\n",
        "STEP_2_PYTHON_SCRIPT=\"First.py\"\n",
        "STEP_3_PYTHON_SCRIPT=\"Second.py\"\n",
        "FINAL_OUTPUT=\"$DATA_DIR/final_results.csv\"\n",
        "\n",
        "wait_for_jobs() {\n",
        "    local job_ids=($1)  # Convert job IDs to an array\n",
        "    local step_name=$2\n",
        "    local start_time=$(date +%s)\n",
        "\n",
        "    echo -n \"⏳ Waiting for $step_name to complete... (00:00:00)\"\n",
        "    \n",
        "    while true; do\n",
        "        all_completed=true  # Assume all jobs are done unless proven otherwise\n",
        "        sleep 1  # Check every 1 second\n",
        "\n",
        "        for job_id in \"${job_ids[@]}\"; do\n",
        "            # Check if the job is still running\n",
        "            if squeue -j \"$job_id\" | grep -q \"$job_id\"; then\n",
        "                all_completed=false  # At least one job is still running\n",
        "                break\n",
        "            fi\n",
        "\n",
        "            # If job is not in squeue, check its historical status in sacct\n",
        "            job_status=$(sacct -j \"$job_id\" --format=State --noheader | awk '{print $1}' | sort | uniq)\n",
        "\n",
        "            if [[ \"$job_status\" =~ \"FAILED|CANCELLED|TIMEOUT\" ]]; then\n",
        "                echo -e \"\\n❌ $step_name failed. Job ID: $job_id, Status: $job_status\"\n",
        "                exit 1\n",
        "            elif [[ \"$job_status\" == \"COMPLETED\" ]]; then\n",
        "                continue  # Job is done, check the next one\n",
        "            elif [[ -z \"$job_status\" ]]; then\n",
        "                echo -e \"\\n⚠️  Warning: Job ID $job_id not found in SLURM history. Assuming completed.\"\n",
        "            else\n",
        "                all_completed=false  # Job status is unclear, so keep waiting\n",
        "                break\n",
        "            fi\n",
        "        done\n",
        "\n",
        "        # If all jobs are completed, exit the loop\n",
        "        if $all_completed; then\n",
        "            break\n",
        "        fi\n",
        "\n",
        "        # Update elapsed time dynamically\n",
        "        current_time=$(date +%s)\n",
        "        elapsed=$((current_time - start_time))\n",
        "        hours=$((elapsed / 3600))\n",
        "        minutes=$(((elapsed % 3600) / 60))\n",
        "        seconds=$((elapsed % 60))\n",
        "        printf \"\\r⏳ Waiting for $step_name to complete... (%02d:%02d:%02d)\" $hours $minutes $seconds\n",
        "    done\n",
        "\n",
        "    # Final message\n",
        "    printf \"\\r✅ $step_name completed in (%02d:%02d:%02d)\\n\" $hours $minutes $seconds\n",
        "}\n",
        "\n",
        "echo \"🚀 Submitting SLURM Jobs for Optimized Pipeline...\"\n",
        "\n",
        "# ===================== STEP 1: SPLIT REVIEWS (PARALLEL JOBS) =====================\n",
        "echo \"📂 Submitting Step 1: Splitting reviews into parallel jobs...\"\n",
        "\n",
        "STEP1_JOB_IDS=()\n",
        "for i in {1..5}; do\n",
        "    STEP1_JOB=$(sbatch --parsable <<EOF\n",
        "#!/bin/bash\n",
        "#SBATCH --job-name=BigDataMiningP1Q2_split_rating_$i\n",
        "#SBATCH --output=split_rating_$i.out\n",
        "#SBATCH --mem=4G\n",
        "#SBATCH --cpus-per-task=1\n",
        "\n",
        "output_file=\"$DATA_DIR/rating_$i.txt\"\n",
        "echo \"user_id,rating,gmap_id\" > \"\\$output_file\"\n",
        "\n",
        "zcat \"$REVIEW_FILE\" | jq --argjson i \"$i\" -r 'select(.rating == $i) | [.user_id, .rating, .gmap_id] | @csv' >> \"\\$output_file\"\n",
        "\n",
        "echo \"✅ Created \\$output_file with \\$(wc -l < \"\\$output_file\") lines.\"\n",
        "EOF\n",
        ")\n",
        "    STEP1_JOB_IDS+=(\"$STEP1_JOB\")\n",
        "done\n",
        "\n",
        "echo \"📊 Step 1 Jobs Submitted: ${STEP1_JOB_IDS[*]}\"\n",
        "\n",
        "DEPENDENCY_STEP1=$(IFS=,; echo \"${STEP1_JOB_IDS[*]}\")\n",
        "\n",
        "# ===================== WAIT FOR STEP 1 TO FINISH =====================\n",
        "wait_for_jobs \"$DEPENDENCY_STEP1\" \"Step 1 (Splitting Reviews)\"\n",
        "\n",
        "# ===================== STEP 2: COUNT gmap_id (PARALLEL JOBS) =====================\n",
        "echo \"📂 Submitting Step 2: Counting gmap_id occurrences...\"\n",
        "\n",
        "STEP2_JOB_IDS=()\n",
        "for i in {1..5}; do\n",
        "    STEP2_JOB=$(sbatch --parsable --dependency=afterok:$DEPENDENCY_STEP1 <<EOF\n",
        "#!/bin/bash\n",
        "#SBATCH --job-name=BigDataMiningP1Q2_count_rating_$i\n",
        "#SBATCH --output=count_rating_$i.out\n",
        "#SBATCH --mem=4G\n",
        "#SBATCH --cpus-per-task=1\n",
        "\n",
        "input_file=\"$DATA_DIR/rating_$i.txt\"\n",
        "output_file=\"$DATA_DIR/rating_${i}_counts.txt\"\n",
        "\n",
        "if [ -s \"\\$input_file\" ]; then\n",
        "    python3 \"$STEP_2_PYTHON_SCRIPT\" \"\\$input_file\" \"\\$output_file\"\n",
        "\n",
        "    if [ -s \"\\$output_file\" ]; then\n",
        "        echo \"✅ Created \\$output_file with \\$(wc -l < \"\\$output_file\") lines.\"\n",
        "    else\n",
        "        echo \"❌ Error: Output file \\$output_file was not created.\"\n",
        "    fi\n",
        "else\n",
        "    echo \"⚠️ Skipping empty file: \\$input_file\"\n",
        "fi\n",
        "EOF\n",
        ")\n",
        "    STEP2_JOB_IDS+=(\"$STEP2_JOB\")\n",
        "done\n",
        "\n",
        "echo \"📊 Step 2 Jobs Submitted: ${STEP2_JOB_IDS[*]}\"\n",
        "\n",
        "DEPENDENCY_STEP2=$(IFS=,; echo \"${STEP2_JOB_IDS[*]}\")\n",
        "\n",
        "# ===================== WAIT FOR STEP 2 TO FINISH =====================\n",
        "wait_for_jobs \"$DEPENDENCY_STEP2\" \"Step 2 (Counting gmap_id)\"\n",
        "\n",
        "# ===================== STEP 3: AGGREGATE RESULTS =====================\n",
        "echo \"📂 Submitting Step 3: Aggregating final results...\"\n",
        "\n",
        "STEP3_JOB=$(sbatch --parsable --dependency=afterok:$DEPENDENCY_STEP2 <<EOF\n",
        "#!/bin/bash\n",
        "#SBATCH --job-name=BigDataMiningP1Q2_aggregate_results\n",
        "#SBATCH --output=aggregate_results.out\n",
        "#SBATCH --mem=8G\n",
        "#SBATCH --cpus-per-task=2\n",
        "\n",
        "zcat \"$META_FILE\" | python3 \"$STEP_3_PYTHON_SCRIPT\" \"$DATA_DIR\" \"/dev/stdin\" \"$FINAL_OUTPUT\"\n",
        "\n",
        "if [ -s \"$FINAL_OUTPUT\" ]; then\n",
        "    echo \"✅ Step 3 completed successfully. Final results saved in $FINAL_OUTPUT\"\n",
        "else\n",
        "    echo \"❌ Step 3 failed. No output file generated.\"\n",
        "    exit 1\n",
        "fi\n",
        "EOF\n",
        ")\n",
        "\n",
        "echo \"📊 Step 3 Jobs Submitted: $STEP3_JOB\"\n",
        "\n",
        "# ===================== WAIT FOR STEP 3 TO FINISH =====================\n",
        "wait_for_jobs \"$STEP3_JOB\" \"Step 3 (Aggregation)\"\n",
        "\n",
        "# ===================== PRINT TOP 3 BUSINESSES =====================\n",
        "if [ -s \"$FINAL_OUTPUT\" ]; then\n",
        "    echo \"✅ Aggregation complete. Displaying top 3 businesses from $FINAL_OUTPUT\"\n",
        "\n",
        "    echo \"\"\n",
        "    echo \"🏆 Top 3 Businesses:\"\n",
        "    echo \"--------------------------\"\n",
        "    head -n 4 \"$FINAL_OUTPUT\" | tail -n 3 | column -t -s \",\"\n",
        "    echo \"--------------------------\"\n",
        "else\n",
        "    echo \"❌ Error: Aggregation failed or output file is empty.\"\n",
        "fi\n",
        "\n",
        "echo \"✅ Optimized SLURM Pipeline Completed Successfully.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "305ada1e",
      "metadata": {},
      "source": [
        "Dear Bodek, on your local terminal, run sftp in order to upload/update files, and then re-enter the cluster to start submitting the job and track its live prints using the commands below(change my username):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9d36bed",
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "REMOTE_USER=\"nathan.pasder\"\n",
        "REMOTE_SFTP_HOST=\"ftp.rcs.huji.ac.il\"\n",
        "REMOTE_JUMP_HOST=\"bava.cs.huji.ac.il\"\n",
        "REMOTE_DIR=\"/sci/home/nathan.pasder\"\n",
        "LOCAL_DIR=~/Documents/HUJI/BigDataMining52002/Final/\n",
        "\n",
        "echo \"🚀 Uploading new scripts via SFTP...\"\n",
        "sftp -J \"$REMOTE_USER@$REMOTE_JUMP_HOST\" \"$REMOTE_USER@$REMOTE_SFTP_HOST\" << EOF\n",
        "    lcd \"$LOCAL_DIR\"\n",
        "    cd \"$REMOTE_DIR\"\n",
        "\n",
        "    # Ensure Data directory exists\n",
        "    mkdir Data 2>/dev/null  # Won't fail if it already exists\n",
        "\n",
        "    # Upload new scripts (will overwrite existing files)\n",
        "    lcd \"$LOCAL_DIR\"\n",
        "    put main_bash.sh\n",
        "    put First.py\n",
        "    put Second.py\n",
        "\n",
        "    echo \"✅ Upload complete.\"\n",
        "    bye\n",
        "EOF\n",
        "\n",
        "# ===================== EXECUTE JOB ON REMOTE =====================\n",
        "ssh -J \"$REMOTE_USER@$REMOTE_JUMP_HOST\" \"$REMOTE_USER@moriah-gw.cs.huji.ac.il\"\n",
        "ls\n",
        "sbatch main_bash.sh\n",
        "squeue -u $USER\n",
        "tail -f bigdata_pipeline.out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "271eaa10",
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "output:\n",
        "\n",
        "```\n",
        "nathan.pasder@moriah-gw-02:~ $ ls \n",
        "Data/  First.py  Second.py*  main_bash.sh*\n",
        "nathan.pasder@moriah-gw-02:~ $ sbatch main_bash.sh\n",
        "\n",
        "Submitted batch job 29600284\n",
        "nathan.pasder@moriah-gw-02:~ $ squeue -u $USER\n",
        "\n",
        "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
        "          29600285   glacier BigDataM nathan.p  R       0:05      1 glacier-28\n",
        "          29600286   glacier BigDataM nathan.p  R       0:05      1 glacier-29\n",
        "          29600287   glacier BigDataM nathan.p  R       0:05      1 glacier-38\n",
        "          29600288   glacier BigDataM nathan.p  R       0:05      1 glacier-38\n",
        "          29600289   glacier BigDataM nathan.p  R       0:05      1 glacier-38\n",
        "          29600284   glacier BigDataM nathan.p  R       0:06      1 glacier-29\n",
        "nathan.pasder@moriah-gw-02:~ $ tail -f bigdata_pipeline.out\n",
        "\n",
        "🚀 Submitting SLURM Jobs for Optimized Pipeline...\n",
        "📂 Submitting Step 1: Splitting reviews into parallel jobs...\n",
        "📊 Step 1 Jobs Submitted: 29600285 29600286 29600287 29600288 29600289\n",
        "✅ Step 1 (Splitting Reviews) completed in (00:02:08)... (00:02:08)\n",
        "📂 Submitting Step 2: Counting gmap_id occurrences...\n",
        "📊 Step 2 Jobs Submitted: 29600297 29600298 29600299 29600300 29600301\n",
        "✅ Step 2 (Counting gmap_id) completed in (00:00:16)... (00:00:16)\n",
        "📂 Submitting Step 3: Aggregating final results...\n",
        "📊 Step 3 Jobs Submitted: 29600302\n",
        "✅ Step 3 (Aggregation) completed in (00:00:13)... (00:00:13)\n",
        "✅ Aggregation complete. Displaying top 3 businesses from Data/final_results.csv\n",
        "\n",
        "🏆 Top 3 Businesses:\n",
        "--------------------------\n",
        "0x87b21fedbf7f4f5b:0xa379fce9177a3dc6  372  5.0  Tip Top K9 Dog Training              \n",
        "0x5495a269266d6e8b:0x8c2044a8b62bc3cb  348  5.0  Peniche and Associates               \n",
        "0x89aec3802de3cd09:0x1bd2662c90a6a12c  268  5.0  \"Pirate and Pixie Dust Destinations   LLC\"\n",
        "--------------------------\n",
        "✅ Optimized SLURM Pipeline Completed Successfully.\n",
        "^C\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WAhUAOUy_ICy",
      "metadata": {
        "id": "WAhUAOUy_ICy"
      },
      "source": [
        "# Part 2 : Streaming Algorithms\n",
        "\n",
        "## Q1. Streaming Sampling Algorithm\n",
        "- Write python function that reads the `review-Oregon.json` file **line-by-line**, i,e, **one line at a time** (see code template below).  Your code should implement  \n",
        "online sampling of 1000 random users and all of their ratings. That is, you should initialize and update a data structue such that\n",
        "after each value of $n$ lines that were processed that correspond to $k \\leq n$ distinct users, it should hold that your data structure stores the identity of $min(k, 1000)$ users chosen uniformly at random from the first $k$ users (without replacement), and will also store **all** lines corresponding to these users\n",
        "\n",
        "- After finishing to process all lines in the file, compute 'ave_rating' for all buisnesses using this sample of $1000$ users, and make a scatter plot of this `ave_rating` vs. the `ave_rating` from Unix Q2 using all users. Decsribe the results\n",
        "\n",
        "**Notes:** You should never store the entire file in memory. After reading each line, if you decide to not include the corresponding user in your sample you should throw it away and never use it again.\n",
        "Exclude from the scatter plot buisnesses that were not reviewed by any of the 1000 users in your sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae063dee",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "\n",
        "def online_user_sampling(file_path, sample_size=1000):\n",
        "    \"\"\"\n",
        "    Reads the file line by line, maintains a random sample of 1000 users, \n",
        "    and stores all their reviews.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n🚀 Starting Online User Sampling...\", flush=True)\n",
        "    start_time = time.time()\n",
        "\n",
        "    user_reviews = {}  # Dictionary to store selected user reviews\n",
        "    unique_users = set()  # Set to track unique users encountered\n",
        "    processed_lines = 0  # Counter for processed lines\n",
        "    replaced_users = 0  # Counter for replaced users\n",
        "\n",
        "    with open(file_path, \"r\") as file:\n",
        "        for line in file:\n",
        "            processed_lines += 1\n",
        "            \n",
        "            try:\n",
        "                review = json.loads(line.strip())  # Parse JSON line\n",
        "                user_id = review[\"user_id\"]\n",
        "\n",
        "                # First 1000 unique users - store all their reviews\n",
        "                if user_id in user_reviews:\n",
        "                    user_reviews[user_id].append(review)\n",
        "                elif len(unique_users) < sample_size:\n",
        "                    unique_users.add(user_id)\n",
        "                    user_reviews[user_id] = [review]\n",
        "                    print(f\"\\r\\n🆕 Added new user: {user_id} (Total: {len(unique_users)})\", flush=True)\n",
        "                else:\n",
        "                    # Reservoir sampling: Replace existing user with probability (sample_size / processed_users)\n",
        "                    replace_chance = sample_size / len(unique_users)\n",
        "                    if random.random() < replace_chance:\n",
        "                        removed_user = random.choice(list(user_reviews.keys()))  # Remove a random user\n",
        "                        del user_reviews[removed_user]\n",
        "                        unique_users.remove(removed_user)\n",
        "                        replaced_users += 1\n",
        "\n",
        "                        # Add the new user\n",
        "                        unique_users.add(user_id)\n",
        "                        user_reviews[user_id] = [review]\n",
        "                        print(f\"\\r🔄 Replaced user {removed_user} with {user_id} (Total Replacements: {replaced_users})\", flush=True)\n",
        "\n",
        "                # Live status update every 10,000 lines (without cluttering output)\n",
        "                if processed_lines % 10000 == 0:\n",
        "                    print(f\"\\r⏳ Processed {processed_lines} lines | Unique Users Seen: {len(unique_users)} | Replacements: {replaced_users}\", end=\"\", flush=True)\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"\\n❌ Error decoding JSON at line {processed_lines}: {e}\", flush=True)\n",
        "                continue  # Skip malformed lines\n",
        "\n",
        "    # Final statistics\n",
        "    print(\"\\n✅ Sampling Completed!\")\n",
        "    print(f\"📊 Total Processed Lines: {processed_lines}\")\n",
        "    print(f\"👤 Unique Users Encountered: {len(unique_users)}\")\n",
        "    print(f\"🔄 Total Users Replaced: {replaced_users}\")\n",
        "\n",
        "    # Compute statistics on how many reviews each sampled user has\n",
        "    review_counts = [len(reviews) for reviews in user_reviews.values()]\n",
        "    if review_counts:\n",
        "        print(f\"📈 Min Reviews per User: {min(review_counts)}\")\n",
        "        print(f\"📉 Max Reviews per User: {max(review_counts)}\")\n",
        "        print(f\"📊 Avg Reviews per User: {sum(review_counts) / len(review_counts):.2f}\")\n",
        "\n",
        "    print(f\"⏳ Execution Time: {time.time() - start_time:.2f} sec\")\n",
        "\n",
        "    return user_reviews\n",
        "\n",
        "# Run the function on the dataset\n",
        "file_path = \"Data/review-Oregon.json\"\n",
        "sampled_users = online_user_sampling(file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jLIP9BvFlgQT",
      "metadata": {
        "id": "jLIP9BvFlgQT"
      },
      "outputs": [],
      "source": [
        "# Your initialization code here\n",
        "\n",
        "# Open the file in read mode and process it line-by-line.\n",
        "with open(\"review-Oregon.json\", \"r\") as file:\n",
        "    for line in file:\n",
        "        # Add code here to process line and update your sample"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gdH-T-fhljbl",
      "metadata": {
        "id": "gdH-T-fhljbl"
      },
      "source": [
        "# Part 3: Networks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lMc8QAh6h5am",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMc8QAh6h5am",
        "outputId": "8829a27d-7058-4315-ed87-b7010bc6ee93"
      },
      "outputs": [],
      "source": [
        "! pip install python-louvain\n",
        "! pip install folium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CMyl4Y4sFu60",
      "metadata": {
        "id": "CMyl4Y4sFu60"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import community as community_louvain\n",
        "from geopy.distance import geodesic\n",
        "import folium\n",
        "import random\n",
        "from branca.colormap import LinearColormap"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l0Lwtv82MlWd",
      "metadata": {
        "id": "l0Lwtv82MlWd"
      },
      "source": [
        "## Q1. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pSe2zDpdnQ0o",
      "metadata": {
        "id": "pSe2zDpdnQ0o"
      },
      "source": [
        "a. Read the data file `network-table.txt` you created in the the previous Unix question.  \n",
        "In addition, read the file *meta-Oregon.json*, which contains additional information about each `gmap_id`, such as category, website, and more.  \n",
        "Display the first five rows of each dataset and explain what is shown and what does the data represents.\n",
        "\n",
        "**Note:** If you failed to create the correct `network-table.txt` file in the unix part, you can use for this question the file we supply.\n",
        "\n",
        "b. Read the file `network-table.txt` and plot a histogram showing the number of reviews by each user (`user_id`).\n",
        "Next, plot a histogram showing the number of unique users reviewing each buisness (`gmap_id`).\n",
        "\n",
        "c.\n",
        "  - Display the distribution of buisness categories using the `meta-Oregon` file. For each buisness having multiple categories use only the first `category`. Show only the top 30 categories having the largest number of buisnesses. Highlight all the restaurant cateories  in a different color.  \n",
        "  - Choose 4 of the top 30 categories and show for each one of them the distribution of `avg_rating` (from the `meta-Oregon` file) for this cateroy.\n",
        "\n",
        "d. Finally, filter the  the `meta-Oregon` file to include only buisnesses with more than 100 reviews. Use `Folium` to create a map showing the buisnesses with more than 100 reviews as circles, color them by the `avg_rating` between red to green and make their size proportional to the number of reviews (use `radius` = `num_reviews` / 1000).\n",
        "Using all of these, describe the data and explain its meaning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i2P53q4NMtzJ",
      "metadata": {
        "id": "i2P53q4NMtzJ"
      },
      "source": [
        "**Solution**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xRfoLHX7O_ye",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRfoLHX7O_ye",
        "outputId": "a6c2ded4-9008-4a3f-9378-85f84ab6273e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gXajitWYjy1X",
      "metadata": {
        "id": "gXajitWYjy1X"
      },
      "outputs": [],
      "source": [
        "# Load the data and show the tables ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P6Duc7msj8yj",
      "metadata": {
        "id": "P6Duc7msj8yj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "Ohve7JtzUMDh",
      "metadata": {
        "id": "Ohve7JtzUMDh"
      },
      "source": [
        "explnation..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11ISAHLrkAW0",
      "metadata": {
        "id": "11ISAHLrkAW0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "-WMval9NkGoN",
      "metadata": {
        "id": "-WMval9NkGoN"
      },
      "source": [
        "explnation..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RW1u6ZryTnM6",
      "metadata": {
        "id": "RW1u6ZryTnM6"
      },
      "source": [
        "## Q2. Community Detection - The Louvain Algorithm\n",
        "- Explain the Louvain algorithm in words, provide an example with at least 6 nodes, display it on a plot, and mark the communities that are formed in the example according to the algorithm (you can use the `community_louvain` implementation).\n",
        "\n",
        "-  Make one change in the divison to communities (move at least one node from one community to another) and show that the resulting division is sub-optimal by comparing the cost function of the two communities. Write explicitly the formulas that you use."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dGD-xVHnlLfY",
      "metadata": {
        "id": "dGD-xVHnlLfY"
      },
      "source": [
        "**Solution**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0tCRKrdhhEkC",
      "metadata": {
        "id": "0tCRKrdhhEkC"
      },
      "source": [
        "Explaining in words:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "na0_SHP1kXM8",
      "metadata": {
        "id": "na0_SHP1kXM8"
      },
      "outputs": [],
      "source": [
        "# Code that plot the example of 6 nodes devided into communites"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SagiEuNtkmd7",
      "metadata": {
        "id": "SagiEuNtkmd7"
      },
      "source": [
        "Suggest change and show the impact using the formulas..  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ADImk8l6iamc",
      "metadata": {
        "id": "ADImk8l6iamc"
      },
      "source": [
        "## Q3. Network Preliminary Analysis\n",
        "Represent the data file 'network_data.txt' you've loaded as an **undirected** graph using the `gmap_id_from` and `gmap_id_to` fields:\n",
        "Make sure that each undorederd (`gmap_id_from` and `gmap_id_to`) appears only once, and remove self loops.\n",
        "\n",
        "Next, analyze the network:\n",
        "- First, print the number nodes and edges in the graph.\n",
        "- Then, plot the degree distribution and explain what the plot reveals.\n",
        "- Finally, visualize the entire graph and describe what you observe. Use the default layout of `networkx` draw function.\n",
        "\n",
        "**Note: the plot might take a while to run..**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8HyGo4cplM7-",
      "metadata": {
        "id": "8HyGo4cplM7-"
      },
      "source": [
        "**Solution**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SwGcA5BdTTcw",
      "metadata": {
        "id": "SwGcA5BdTTcw"
      },
      "outputs": [],
      "source": [
        "G = nx.Graph()\n",
        "\n",
        "for _, row in network_df.iterrows():\n",
        "    G.add_edge(\n",
        "        # ...\n",
        "       )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YYHXxMtdXgfE",
      "metadata": {
        "id": "YYHXxMtdXgfE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "xOi47Rn9UGhP",
      "metadata": {
        "id": "xOi47Rn9UGhP"
      },
      "source": [
        "## Q4. Network Community Analysis\n",
        "\n",
        "- We want to focus on buisnesses with many reviews. Keep only the nodes that have at least 15 edges. Afterward, remove nodes that are not connected to the central part of the network, i.e. the largest connected component.  \n",
        "\n",
        "You can use the following code to help:  \n",
        "```\n",
        "largest_cc = max(nx.connected_components(subgraph), key=len)  \n",
        "main_component = subgraph.subgraph(largest_cc)  \n",
        "```\n",
        "\n",
        "- Next, run the **Louvain algorithm** to divide the buisnesses in the main connected component into communities. Plot the buisnesses colored by their coomunity and describe the results in detail.\n",
        "\n",
        "- Finally, we want to know whether communities reflect different categories.\n",
        "— For each community compute the fraction of buisnesses form this community in each of the 30 top cateories from Q1, plus a 31st cateorgy called 'other' for all buisnesses in a different category\n",
        "- Plot a heatmap showing the community-by-category fractions.\n",
        "Do you see a relationship between the communities and categories? derive a staitstical test testing the null hypothesis of no such relationship, and report your test results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DpSIoFnjlP76",
      "metadata": {
        "id": "DpSIoFnjlP76"
      },
      "source": [
        "**Solution**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93uY2CQ0fCWE",
      "metadata": {
        "id": "93uY2CQ0fCWE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "jFUofGrUaBaz",
      "metadata": {
        "id": "jFUofGrUaBaz"
      },
      "source": [
        "## Q5. Geographic Community Analysis\n",
        " In this question we will explore whether there is any geographical significance to the communities that were formed.\n",
        "-  Choose the **five largest communities**. For each one of them, calculate the average latitude and longitude $\\text{Lon, Lat}$ of all locations within it.\n",
        "\n",
        "- Then, compute the Empirical Cumulative Distribution Function (**ECDF**) of the distances from each location to the center of buisnesses in this community. Plot in addition the **ECDF** of the distance to the center for all locations in the entire dataset.\n",
        "What do the results show? Do you observe geographical clustering of the communities?  \n",
        "\n",
        "- Next, plot the points of the buisnesses in the largest five communities on a map using the `folium` library, with each community displayed in a different color. What do you observe? Explain in a couple of sentences the results and why they might occur."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uyMh01filWqr",
      "metadata": {
        "id": "uyMh01filWqr"
      },
      "source": [
        "**Solution**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XnHXdgRVldeh",
      "metadata": {
        "id": "XnHXdgRVldeh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
