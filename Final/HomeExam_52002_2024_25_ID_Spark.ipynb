{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c71a501a-e69f-4db7-871f-905e9861fe4a",
      "metadata": {
        "id": "c71a501a-e69f-4db7-871f-905e9861fe4a"
      },
      "source": [
        "# Home Exam 52002 - 2024-2025 - Part 4: Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yi4B9L4mjR8b",
      "metadata": {
        "id": "yi4B9L4mjR8b"
      },
      "source": [
        "#**Instructions**\n",
        "* **Fill your ID Here:** [replace the bracketed text with your ID number]\n",
        "\n",
        "Rename the solution file 'HomeExam_52002_2024_25_[ID]_Spark.ipynb' such that [ID] should be replaced by your ID number.\n",
        "* Submit your filled solution by Febuary 28th 23:59 your solution on moodle.\n",
        "\n",
        "Read carefully all instructions in the Parts 1-3 file, as they apply also here\n",
        "\n",
        "###Submission Guidelines:\n",
        "\n",
        "By the end of the exercise, please submit the following **four** files:\n",
        "\n",
        "\n",
        "1. **Networks, Streaming, Unix, and Batch Task Processing:**  \n",
        "   - Provide your solutions in both `.ipynb` (Jupyter Notebook) and `.html` formats. Submit after running all parts of the `.ipynb` notebook except the unix part , check that the outputs of each question were created and saved. For the unix part, copy the code and results manually to the `.ipynb` notebook.\n",
        "\n",
        "2. **Spark Section:**  \n",
        "   - Submit the fully executed Jupyter Notebook (`.ipynb`) with all expected outputs, after running it in the Databricks environment.\n",
        "   - Include an `.html` export of the executed notebook displaying the outputs.  \n",
        "\n",
        "\n",
        "Ensure that all submitted files are clearly labeled and display the required outputs where applicable.\n",
        "\n",
        "* **Good luck!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oyii5rAe21xh",
      "metadata": {
        "id": "oyii5rAe21xh"
      },
      "source": [
        "# Part 4: Spark - 25 points"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wZZhsWmxkUV9",
      "metadata": {
        "id": "wZZhsWmxkUV9"
      },
      "source": [
        "## Q1. Upload the data and preliminary analysis (_ points)\n",
        "\n",
        "Login into you account- use the **community** login! [link]( https://community.cloud.databricks.com/login.html)\n",
        "\n",
        "In this part we will work with spark, in order to upload files into Databricks you may look at the [next guide](https://www.youtube.com/watch?v=7zT9GKtBVHM)\n",
        "\n",
        "Use the `network_review_Oregon.json` from the first section as data. Run the next query in order to check that the upload was successful\n",
        "\n",
        "Use RDD operations and MAP-Reduce Spark commands using on the `network_rdd` variable to\n",
        "1.  Find the top 3 users who posted the earliest reviews in the data. What does this number represent (`time`) and what is the data in human language?\n",
        "2.  Find the top 3 users with the highest number of reviews\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "xwWIB5Y-e3UA",
      "metadata": {
        "id": "xwWIB5Y-e3UA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%python` not found (But cell magic `%%python` exists, did you mean that instead?).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%python\n",
        "# Run this code to load the data first\n",
        "import json\n",
        "\n",
        "network_rdd = sc.textFile(\"/FileStore/tables/final_Term/network_review_Oregon.json\") \\\n",
        "        .map(lambda x: json.loads(x))\n",
        "print(network_rdd.take(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0KQs0t42jNHi",
      "metadata": {
        "id": "0KQs0t42jNHi"
      },
      "source": [
        "**Solution**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-ahrNeGJjOXP",
      "metadata": {
        "id": "-ahrNeGJjOXP"
      },
      "source": [
        "1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "LM3PEwBRhbAv",
      "metadata": {
        "id": "LM3PEwBRhbAv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in ./.venv/lib/python3.13/site-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in ./.venv/lib/python3.13/site-packages (from pyspark) (0.10.9.7)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "[{'user_id': '113524129708146481732', 'name': 'Alyssa Miller', 'time': 1534099975802, 'rating': 4, 'text': None, 'pics': None, 'resp': None, 'gmap_id': '0x54950a0305f0bc7d:0xd427df7d61f305d3'}, {'user_id': '114476130197618351498', 'name': 'ColdGhost', 'time': 1605617141349, 'rating': 5, 'text': 'An older gentleman helped us find the exact washers and screws we needed and all the staff was extremely pleasant and helpful great place really.', 'pics': None, 'resp': None, 'gmap_id': '0x549576bf17534d6b:0xa44ebc408880ae85'}, {'user_id': '114675375479747577746', 'name': 'Maria Ortiz', 'time': 1472189695563, 'rating': 5, 'text': None, 'pics': None, 'resp': None, 'gmap_id': '0x549576b3224401e1:0xb9a9291588ca7ef9'}, {'user_id': '106743434380913011087', 'name': 'Dustin Wright', 'time': 1562774566765, 'rating': 5, 'text': 'Dimitri is a stand-up guy and he runs a good operation.', 'pics': None, 'resp': None, 'gmap_id': '0x5495a74d1f9039a1:0xa71b6af993778b05'}, {'user_id': '110130949048638493084', 'name': 'Kao Saetern', 'time': 1621388939983, 'rating': 4, 'text': None, 'pics': None, 'resp': None, 'gmap_id': '0x54c0049e918198ef:0xd63583a68296a2a3'}, {'user_id': '109076620884363591580', 'name': 'Blaine Clark', 'time': 1498616119149, 'rating': 5, 'text': 'Great food!  And great prople.', 'pics': None, 'resp': None, 'gmap_id': '0x54c57f837fc9ec39:0x996766af689e79f7'}, {'user_id': '108717089078854331166', 'name': 'William Hall', 'time': 1533053401819, 'rating': 5, 'text': 'Friendly', 'pics': None, 'resp': None, 'gmap_id': '0x54c0fef5d7f24d73:0x5c77f0a90640f504'}, {'user_id': '108773633302141249199', 'name': 'Cynthia Fagaly', 'time': 1594666674775, 'rating': 5, 'text': \"Excellent customer service, Dairy Queen store. And I just love buying tools cuz I'd like to fix my own things and create projects to build things.\", 'pics': None, 'resp': None, 'gmap_id': '0x549510170d86246d:0xdee25b29ddc2716f'}, {'user_id': '110712482315678206928', 'name': 'M E', 'time': 1537414120983, 'rating': 5, 'text': 'Absolutely one of our favorite \"goto\" places in the Hillsboro. Wonderful service, wonderful people, and terrific food.', 'pics': None, 'resp': None, 'gmap_id': '0x5495054fdf52b48b:0x2339b2f21f57762e'}, {'user_id': '118318594715738057959', 'name': 'Michael Blamphi', 'time': 1546718457664, 'rating': 5, 'text': \"This place is amazing! I was totally blown away by their Cuban sandwich, which is what brought me over. Also tried their chicken parmesan they were running as a special and hoooollyy smokes... It's damn good. Super nice fellas workin the cart too. A+ service. Kudos. They sell out pretty regularly, took me 4 attempts to eatthere before I did, so I recommend getting there as early as possible!\", 'pics': None, 'resp': None, 'gmap_id': '0x5495a0edc896c78d:0xc8fa487b0b738e8d'}]\n",
            "\n",
            "Top 3 users with the earliest reviews:\n",
            "Name: Marc Lippman, Time: 1208476800000 (which is 2008-04-18 03:00:00)\n",
            "Name: paulie campbell, Time: 1208881092090 (which is 2008-04-22 19:18:12.090000)\n",
            "Name: Michael Dorausch, Time: 1214857301974 (which is 2008-06-30 23:21:41.974000)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import json\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "        .appName(\"LocalSparkApp\") \\\n",
        "        .getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "network_rdd = sc.textFile(\"Data/network-review-Oregon.json\") \\\n",
        "        .map(lambda x: json.loads(x))\n",
        "\n",
        "print(network_rdd.take(10))\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "# Get (user, time) pairs, sort by time (earliest first) and take top 3\n",
        "earliest_reviews = network_rdd.map(lambda record: (record['name'], record['time'])) \\\n",
        "                              .sortBy(lambda x: x[1]) \\\n",
        "                              .take(3)\n",
        "\n",
        "print(\"\\nTop 3 users with the earliest reviews:\")\n",
        "for name, timestamp in earliest_reviews:\n",
        "    # Convert from milliseconds to seconds\n",
        "    human_time = datetime.fromtimestamp(timestamp / 1000)\n",
        "    print(f\"Name: {name}, Time: {timestamp} (which is {human_time})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37hxL5mNjLLp",
      "metadata": {
        "id": "37hxL5mNjLLp"
      },
      "source": [
        "2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "d8Fy1NcnjQMH",
      "metadata": {
        "id": "d8Fy1NcnjQMH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 3 users with the highest number of reviews:\n",
            "Name: A Google User, Number of reviews: 69\n",
            "Name: Google User, Number of reviews: 22\n",
            "Name: Matt, Number of reviews: 21\n"
          ]
        }
      ],
      "source": [
        "# Create (user, 1) pairs for each review and then count reviews per user\n",
        "user_review_count = network_rdd.map(lambda record: (record['name'], 1)) \\\n",
        "                               .reduceByKey(lambda count1, count2: count1 + count2)\n",
        "\n",
        "# Sort by the count in descending order and take the top 3\n",
        "top_reviewers = user_review_count.sortBy(lambda x: x[1], ascending=False).take(3)\n",
        "\n",
        "print(\"Top 3 users with the highest number of reviews:\")\n",
        "for name, count in top_reviewers:\n",
        "    print(f\"Name: {name}, Number of reviews: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6RKbP5tBgeFx",
      "metadata": {
        "id": "6RKbP5tBgeFx"
      },
      "source": [
        "## Q2. Calculate Average rating (_ points)  \n",
        "Use RDD operations (Map-Reduce) to  calculate the `avg_rating` for each `gmap_id`. Show the top three `gmap_id` sorted by `avg_rating` and the number of reviews (both in descending order).  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2WNfJNRx6KTD",
      "metadata": {
        "id": "2WNfJNRx6KTD"
      },
      "source": [
        "**Solution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "SQDAzTzOes_i",
      "metadata": {
        "id": "SQDAzTzOes_i"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 3 gmap_id by average rating and review count:\n",
            "gmap_id: 0x54956d7d623d3741:0x36f9814537de2d8e, Average Rating: 5.00, Number of Reviews: 38\n",
            "gmap_id: 0x88d92749d865bfc7:0x386f8a39d715d4e2, Average Rating: 5.00, Number of Reviews: 32\n",
            "gmap_id: 0x88f5a391e525f70b:0x5dec0de9516af60c, Average Rating: 5.00, Number of Reviews: 30\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Map each record to (gmap_id, (rating, 1))\n",
        "gmap_rating_pairs = network_rdd.map(lambda record: (record['gmap_id'], (record['rating'], 1)))\n",
        "\n",
        "# Step 2: Reduce by key to sum ratings and counts\n",
        "gmap_rating_aggregated = gmap_rating_pairs.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
        "\n",
        "# Step 3: Calculate average rating for each gmap_id\n",
        "gmap_avg_rating = gmap_rating_aggregated.map(lambda kv: (kv[0], (kv[1][0] / kv[1][1], kv[1][1])))\n",
        "\n",
        "# Step 4: Sort by average rating and number of reviews, both in descending order\n",
        "# The key for sorting is a tuple (avg_rating, total_reviews)\n",
        "sorted_gmap = gmap_avg_rating.sortBy(lambda kv: (kv[1][0], kv[1][1]), ascending=False)\n",
        "\n",
        "# Step 5: Take the top three results\n",
        "top_three_gmaps = sorted_gmap.take(3)\n",
        "\n",
        "print(\"Top 3 gmap_id by average rating and review count:\")\n",
        "for gmap_id, (avg_rating, total_reviews) in top_three_gmaps:\n",
        "    print(f\"gmap_id: {gmap_id}, Average Rating: {avg_rating:.2f}, Number of Reviews: {total_reviews}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_m0aWS3adux8",
      "metadata": {
        "id": "_m0aWS3adux8"
      },
      "source": [
        "## Q3. Multiplying Sparse Matrices - (_ points)\n",
        "\n",
        "In the exam's data files, you'll find two files with sparse representations of matrices \\( A \\) and \\( B \\), called `A.txt` and `B.txt`.\n",
        "Write Spark code to compute the product of these two matrices using only basic RDD operations. In addition, find and display the highest entry of the product $AB$. Explain the code implementation.\n",
        "\n",
        "For guidance, you can refer to this [blog post](https://bmeyers.github.io/Spark_sparse_matrix_mult/), which provides a detailed implementation of matrix-vector multiplication."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f01XZoKJ6N9z",
      "metadata": {
        "id": "f01XZoKJ6N9z"
      },
      "source": [
        "**Solution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "Fs8IPcDm3WfB",
      "metadata": {
        "id": "Fs8IPcDm3WfB"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 78:>                                                         (0 + 4) / 4]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Highest entry in product AB:\n",
            "((99, 99), 98990000.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Step 1: Load matrix A and matrix B from text files.\n",
        "# Now using comma as the delimiter.\n",
        "A = sc.textFile(\"Data/A.txt\") \\\n",
        "      .map(lambda line: line.split(\",\")) \\\n",
        "      .map(lambda tokens: (int(tokens[0]), int(tokens[1]), float(tokens[2])))\n",
        "\n",
        "B = sc.textFile(\"Data/B.txt\") \\\n",
        "      .map(lambda line: line.split(\",\")) \\\n",
        "      .map(lambda tokens: (int(tokens[0]), int(tokens[1]), float(tokens[2])))\n",
        "\n",
        "# Step 2: Re-key the matrices on the join key j.\n",
        "# For matrix A, key by its column index j.\n",
        "A_keyed = A.map(lambda x: (x[1], (x[0], x[2])))  # (j, (i, A_ij))\n",
        "# For matrix B, key by its row index j.\n",
        "B_keyed = B.map(lambda x: (x[0], (x[1], x[2])))  # (j, (k, B_jk))\n",
        "\n",
        "# Step 3: Join on the common key j.\n",
        "joined = A_keyed.join(B_keyed)\n",
        "# Now, each record is of the form:\n",
        "# (j, ((i, A_ij), (k, B_jk)))\n",
        "\n",
        "# Step 4: For each joined record, compute the partial product.\n",
        "# Re-map to key by (i, k) and multiply the values.\n",
        "partial_products = joined.map(lambda x: ((x[1][0][0], x[1][1][0]), x[1][0][1] * x[1][1][1]))\n",
        "# Each record now is: ((i, k), A_ij * B_jk)\n",
        "\n",
        "# Step 5: Sum up the partial products for each (i, k) to get the final product AB.\n",
        "AB = partial_products.reduceByKey(lambda x, y: x + y)\n",
        "# AB now holds entries of the matrix product, i.e., ((i, k), value)\n",
        "\n",
        "# Step 6: Find the highest entry in the product AB.\n",
        "max_entry = AB.max(key=lambda x: x[1])\n",
        "print(\"Highest entry in product AB:\")\n",
        "print(max_entry)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
